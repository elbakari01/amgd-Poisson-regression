{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1833316d-88a3-4829-ae09-69ccf769d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Adaptive momentum gradient descent \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy import special\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import expit  \n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# matplotlib style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Clipping function\n",
    "def clip(x, threshold=None):\n",
    "    if threshold is None:\n",
    "        return x\n",
    "    return np.clip(x, -threshold, threshold)\n",
    "\n",
    "#Poisson log-likelihood function \n",
    "def poisson_log_likelihood(beta, X, y):\n",
    "    \"\"\"\n",
    "     negative Poisson log-likelihood\n",
    "    \"\"\"\n",
    "    linear_pred = X @ beta\n",
    "    linear_pred = np.clip(linear_pred, -20, 20)\n",
    "    mu = np.exp(linear_pred)\n",
    "    \n",
    "    log_likelihood = np.sum(y * linear_pred - mu - special.gammaln(y + 1))\n",
    "    \n",
    "    return -log_likelihood  # Negative because we want to minimize the function\n",
    "\n",
    "# Evaluation metrics function \n",
    "def evaluate_model(beta, X, y, target_name='Target'):\n",
    "    \"\"\"\n",
    "    Evaluate model performance for a single target\n",
    "    \"\"\"\n",
    "    linear_pred = X @ beta\n",
    "    linear_pred = np.clip(linear_pred, -20, 20)\n",
    "    y_pred = np.exp(linear_pred)\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    mae = np.mean(np.abs(y - y_pred))\n",
    "    \n",
    "    # Root Mean Squared Error\n",
    "    rmse = np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "    \n",
    "    # Mean Poisson Deviance\n",
    "    eps = 1e-10  # To avoid log(0)\n",
    "    deviance = 2 * np.sum(y * np.log((y + eps) / (y_pred + eps)) - (y - y_pred))\n",
    "    mean_deviance = deviance / len(y)\n",
    "    \n",
    "    results = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'Mean Deviance': mean_deviance,\n",
    "        'Non-zero coeffs': np.sum(np.abs(beta) > 1e-6),\n",
    "        'Sparsity': 1.0 - (np.sum(np.abs(beta) > 1e-6) / len(beta))\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "    \n",
    "#AMGD implementation \n",
    "def amgd(X, y, alpha=0.001, beta1=0.8, beta2=0.999, \n",
    "         lambda1=0.1, lambda2=0.0, penalty='l1',\n",
    "         T=20.0, tol=1e-6, max_iter=1000, eta=0.0001, epsilon=1e-8, \n",
    "         verbose=False, return_iters=False):\n",
    "    \"\"\"\n",
    "    Adaptive Momentum Gradient Descent (AMGD) for single-target Poisson regression\n",
    "    with L1 or Elastic Net regularization\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initializing coefficient vector\n",
    "    beta = np.random.normal(0, 0.1, n_features)\n",
    "    \n",
    "    # Initializing momentum variables\n",
    "    m = np.zeros(n_features)\n",
    "    v = np.zeros(n_features)\n",
    "    \n",
    "    prev_loss = float('inf')\n",
    "    loss_history = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Tracking non-zero coefficients for debugging\n",
    "    nonzero_history = []\n",
    "    \n",
    "    # Tracking values at each iteration \n",
    "    if return_iters:\n",
    "        beta_history = []\n",
    "    \n",
    "    for t in range(1, max_iter + 1):\n",
    "        alpha_t = alpha / (1 + eta * t)\n",
    "        \n",
    "        # Computing predictions and gradient\n",
    "        linear_pred = X @ beta\n",
    "        linear_pred = np.clip(linear_pred, -20, 20)\n",
    "        mu = np.exp(linear_pred)\n",
    "        \n",
    "        # Gradient of negative log-likelihood\n",
    "        grad_ll = X.T @ (mu - y)\n",
    "        \n",
    "        # Adding regularization gradient\n",
    "        if penalty == 'l1':\n",
    "            # Pure L1: no gradient term (handled in soft thresholding step)\n",
    "            grad = grad_ll\n",
    "        elif penalty == 'elasticnet':\n",
    "            # Elastic Net: add gradient of L2 component\n",
    "            grad = grad_ll + lambda2 * beta\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown penalty: {penalty}\")\n",
    "        \n",
    "        grad = clip(grad, T)\n",
    "        \n",
    "        # Momentum updates\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        # Parameter update\n",
    "        beta = beta - alpha_t * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        # Apply appropriate regularization\n",
    "        if penalty == 'l1' or penalty == 'elasticnet':\n",
    "            # Adaptive soft-thresholding for L1 component\n",
    "            denom = np.abs(beta) + 0.1\n",
    "            beta = np.sign(beta) * np.maximum(np.abs(beta) - alpha_t * lambda1 / denom, 0)\n",
    "\n",
    "\n",
    "        \n",
    "        # Compute loss\n",
    "        ll = poisson_log_likelihood(beta, X, y)\n",
    "        \n",
    "        # Add regularization component to loss\n",
    "        reg_pen = 0\n",
    "        if penalty == 'l1':\n",
    "            reg_pen = lambda1 * np.sum(np.abs(beta))\n",
    "        elif penalty == 'elasticnet':\n",
    "            reg_pen = lambda1 * np.sum(np.abs(beta)) + (lambda2 / 2) * np.sum(beta**2)\n",
    "        \n",
    "        total_loss = ll + reg_pen\n",
    "        loss_history.append(total_loss)\n",
    "        \n",
    "        # Tracking non-zero coefficients\n",
    "        non_zeros = np.sum(np.abs(beta) > 1e-6)\n",
    "        nonzero_history.append(non_zeros)\n",
    "        \n",
    "        # Tracking beta values \n",
    "        if return_iters:\n",
    "            beta_history.append(beta.copy())\n",
    "        \n",
    "        if verbose and t % 100 == 0:\n",
    "            print(f\"Iteration {t}, Loss: {total_loss:.4f}, Log-likelihood: {ll:.4f}, Penalty: {reg_pen:.4f}\")\n",
    "            print(f\"Non-zero coefficients: {non_zeros}/{n_features}, Sparsity: {1-non_zeros/n_features:.4f}\")\n",
    "        \n",
    "        # Checking convergence\n",
    "        if abs(prev_loss - total_loss) < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged at iteration {t}\")\n",
    "            break\n",
    "            \n",
    "        prev_loss = total_loss\n",
    "    \n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    if return_iters:\n",
    "        return beta, loss_history, runtime, nonzero_history, beta_history\n",
    "    else:\n",
    "        return beta, loss_history, runtime, nonzero_history\n",
    "\n",
    "# AdaGrad implementation with L1 or Elastic Net regularization\n",
    "def adagrad(X, y, alpha=0.01, lambda1=0.1, lambda2=0.0, penalty='l1',\n",
    "            tol=1e-6, max_iter=1000, epsilon=1e-8, verbose=False, return_iters=False):\n",
    "    \"\"\"\n",
    "    AdaGrad optimizer for single-target Poisson regression\n",
    "    with L1 or Elastic Net regularization\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize coefficient vector\n",
    "    beta = np.random.normal(0, 0.1, n_features)\n",
    "    \n",
    "    # Initializing accumulator for squared gradients\n",
    "    G = np.zeros(n_features)\n",
    "    \n",
    "    prev_loss = float('inf')\n",
    "    loss_history = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Tracking non-zero coefficients\n",
    "    nonzero_history = []\n",
    "    \n",
    "    # Tracking values at each iteration \n",
    "    if return_iters:\n",
    "        beta_history = []\n",
    "    \n",
    "    for t in range(1, max_iter + 1):\n",
    "        # Computing predictions and gradient\n",
    "        linear_pred = X @ beta\n",
    "        linear_pred = np.clip(linear_pred, -20, 20)\n",
    "        mu = np.exp(linear_pred)\n",
    "        \n",
    "        # Gradient of negative log-likelihood\n",
    "        grad_ll = X.T @ (mu - y)\n",
    "        \n",
    "        # Add regularization gradient\n",
    "        if penalty == 'l1':\n",
    "            # Pure L1: add subgradient of L1 penalty\n",
    "            grad = grad_ll + lambda1 * np.sign(beta)\n",
    "        elif penalty == 'elasticnet':\n",
    "            # Elastic Net: add combined gradient\n",
    "            grad = grad_ll + lambda1 * np.sign(beta) + lambda2 * beta\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown penalty: {penalty}\")\n",
    "        \n",
    "        # Update accumulator\n",
    "        G += grad ** 2\n",
    "        \n",
    "        # Parameter update with AdaGrad scaling\n",
    "        beta = beta - alpha * grad / (np.sqrt(G) + epsilon)\n",
    "        \n",
    "        # Apply proximal operator for L1 regularization \n",
    "        if penalty == 'l1' or penalty == 'elasticnet':\n",
    "            beta = np.sign(beta) * np.maximum(np.abs(beta) - lambda1 * alpha / (np.sqrt(G) + epsilon), 0)\n",
    "        \n",
    "        # Compute loss\n",
    "        ll = poisson_log_likelihood(beta, X, y)\n",
    "        \n",
    "        # Add regularization component to loss\n",
    "        reg_pen = 0\n",
    "        if penalty == 'l1':\n",
    "            reg_pen = lambda1 * np.sum(np.abs(beta))\n",
    "        elif penalty == 'elasticnet':\n",
    "            reg_pen = lambda1 * np.sum(np.abs(beta)) + (lambda2 / 2) * np.sum(beta**2)\n",
    "        \n",
    "        total_loss = ll + reg_pen\n",
    "        loss_history.append(total_loss)\n",
    "        \n",
    "        # Tracking non-zero coefficients\n",
    "        non_zeros = np.sum(np.abs(beta) > 1e-6)\n",
    "        nonzero_history.append(non_zeros)\n",
    "        \n",
    "        # Tracking beta values \n",
    "        if return_iters:\n",
    "            beta_history.append(beta.copy())\n",
    "        \n",
    "        if verbose and t % 100 == 0:\n",
    "            print(f\"Iteration {t}, Loss: {total_loss:.4f}, Log-likelihood: {ll:.4f}, Penalty: {reg_pen:.4f}\")\n",
    "            print(f\"Non-zero coefficients: {non_zeros}/{n_features}, Sparsity: {1-non_zeros/n_features:.4f}\")\n",
    "        \n",
    "        # Checking convergence\n",
    "        if abs(prev_loss - total_loss) < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged at iteration {t}\")\n",
    "            break\n",
    "            \n",
    "        prev_loss = total_loss\n",
    "    \n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    if return_iters:\n",
    "        return beta, loss_history, runtime, nonzero_history, beta_history\n",
    "    else:\n",
    "        return beta, loss_history, runtime, nonzero_history\n",
    "\n",
    "\n",
    "\n",
    "# Adam implementation with L1 or Elastic Net regularization\n",
    "def adam(X, y, alpha=0.001, beta1=0.9, beta2=0.999, \n",
    "         lambda1=0.1, lambda2=0.0, penalty='l1',\n",
    "         tol=1e-6, max_iter=1000, epsilon=1e-8, verbose=False, return_iters=False):\n",
    "    \"\"\"\n",
    "    Adam optimizer for single-target Poisson regression\n",
    "    with L1 or Elastic Net regularization\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initializing coefficient vector\n",
    "    beta = np.random.normal(0, 0.1, n_features)\n",
    "    \n",
    "    # Initialize moment estimates\n",
    "    m = np.zeros(n_features)  # First moment estimate\n",
    "    v = np.zeros(n_features)  # Second moment estimate\n",
    "    \n",
    "    prev_loss = float('inf')\n",
    "    loss_history = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Tracking non-zero coefficients\n",
    "    nonzero_history = []\n",
    "    \n",
    "    # Track values at each iteration \n",
    "    if return_iters:\n",
    "        beta_history = []\n",
    "    \n",
    "    for t in range(1, max_iter + 1):\n",
    "        # Compute predictions and gradient\n",
    "        linear_pred = X @ beta\n",
    "        linear_pred = np.clip(linear_pred, -20, 20)\n",
    "        mu = np.exp(linear_pred)\n",
    "        \n",
    "        # Gradient of negative log-likelihood\n",
    "        grad_ll = X.T @ (mu - y)\n",
    "        \n",
    "        # Add regularization gradient\n",
    "        if penalty == 'l1':\n",
    "            # Pure L1:  Subgradient for non-zero elements\n",
    "            grad = grad_ll + lambda1 * np.sign(beta) * (np.abs(beta) > 0)\n",
    "        elif penalty == 'elasticnet':\n",
    "            # Elastic Net:  Combined gradient\n",
    "            grad = grad_ll + lambda1 * np.sign(beta) * (np.abs(beta) > 0) + lambda2 * beta\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown penalty: {penalty}\")\n",
    "        \n",
    "        # Updating biased first moment estimate\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        # Updating biased second raw moment estimate\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        \n",
    "        # Computing bias-corrected first moment estimate\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        # Computing bias-corrected second raw moment estimate\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        # Updating parameters\n",
    "        beta = beta - alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        # Apply proximal operator for L1 regularization \n",
    "        if penalty == 'l1' or penalty == 'elasticnet':\n",
    "            beta = np.sign(beta) * np.maximum(np.abs(beta) - lambda1 * alpha, 0)\n",
    "        \n",
    "        # Compute loss\n",
    "        ll = poisson_log_likelihood(beta, X, y)\n",
    "        \n",
    "        # Adding regularization component to loss\n",
    "        reg_pen = 0\n",
    "        if penalty == 'l1':\n",
    "            reg_pen = lambda1 * np.sum(np.abs(beta))\n",
    "        elif penalty == 'elasticnet':\n",
    "            reg_pen = lambda1 * np.sum(np.abs(beta)) + (lambda2 / 2) * np.sum(beta**2)\n",
    "        \n",
    "        total_loss = ll + reg_pen\n",
    "        loss_history.append(total_loss)\n",
    "        \n",
    "        # Tracking non-zero coefficients\n",
    "        non_zeros = np.sum(np.abs(beta) > 1e-6)\n",
    "        nonzero_history.append(non_zeros)\n",
    "        \n",
    "        # Tracking beta values\n",
    "        if return_iters:\n",
    "            beta_history.append(beta.copy())\n",
    "        \n",
    "        if verbose and t % 100 == 0:\n",
    "            print(f\"Iteration {t}, Loss: {total_loss:.4f}, Log-likelihood: {ll:.4f}, Penalty: {reg_pen:.4f}\")\n",
    "            print(f\"Non-zero coefficients: {non_zeros}/{n_features}, Sparsity: {1-non_zeros/n_features:.4f}\")\n",
    "        \n",
    "        # Checking convergence\n",
    "        if abs(prev_loss - total_loss) < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged at iteration {t}\")\n",
    "            break\n",
    "            \n",
    "        prev_loss = total_loss\n",
    "    \n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    if return_iters:\n",
    "        return beta, loss_history, runtime, nonzero_history, beta_history\n",
    "    else:\n",
    "        return beta, loss_history, runtime, nonzero_history\n",
    "\n",
    "##GLM implementation\n",
    "\n",
    "def glmnet(\n",
    "    X, y,\n",
    "    alpha=1.0,\n",
    "    lambda1=1.0,\n",
    "    lambda2=1.0,\n",
    "    penalty='elasticnet',\n",
    "    tol=1e-4,\n",
    "    max_iter=1000,\n",
    "    fit_intercept=False,\n",
    "    verbose=False,\n",
    "    epsilon=1e-8, \n",
    "    return_iters=False,\n",
    "    is_pre_scaled=False,\n",
    "    lr_schedule='inverse_time',  # Options: 'constant', 'inverse_time', 'exponential', 'step'\n",
    "    initial_lr=0.1,\n",
    "    decay_rate=0.01,\n",
    "    step_size=100,  # For step decay\n",
    "    step_factor=0.5  # For step decay\n",
    "):\n",
    "    \"\"\"\n",
    "   Glmnet\n",
    "    \"\"\"\n",
    "    # Standardizing features\n",
    "    if not is_pre_scaled:\n",
    "        scaler = StandardScaler()\n",
    "        X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Adding intercept column\n",
    "    if fit_intercept:\n",
    "        X = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    n_samples, n_features = X.shape\n",
    "    beta = np.zeros(n_features)  # Initializing coefficients\n",
    "    \n",
    "    # Computing regularization parameters\n",
    "    total_penalty = lambda1 + lambda2\n",
    "    if penalty == 'l1':\n",
    "        l1_ratio = 1.0\n",
    "    elif penalty == 'l2':\n",
    "        l1_ratio = 0.0\n",
    "    elif penalty == 'elasticnet':\n",
    "        l1_ratio = lambda1 / total_penalty if total_penalty > 0 else 0.0\n",
    "    else:  # 'none'\n",
    "        l1_ratio = 0.0\n",
    "        total_penalty = 0.0\n",
    "    \n",
    "    # Tracking variables\n",
    "    loss_history = []\n",
    "    beta_history = []\n",
    "    nonzero_history = []\n",
    "    lr_history = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Function to calculate learning rate based on the selected schedule\n",
    "    def get_learning_rate(iteration):\n",
    "        if lr_schedule == 'constant':\n",
    "            return initial_lr\n",
    "        elif lr_schedule == 'inverse_time':\n",
    "            return initial_lr / (1.0 + decay_rate * iteration)\n",
    "        elif lr_schedule == 'exponential':\n",
    "            return initial_lr * np.exp(-decay_rate * iteration)\n",
    "        elif lr_schedule == 'step':\n",
    "            return initial_lr * (step_factor ** (iteration // step_size))\n",
    "        else:\n",
    "            return initial_lr  # Default to constant \n",
    "    \n",
    "    # Main optimization loop\n",
    "    for iteration in range(max_iter):\n",
    "        # Storing current coefficients for convergence check\n",
    "        old_beta = beta.copy()\n",
    "        \n",
    "        # Compute linear predictor and expected values \n",
    "        eta = X @ beta\n",
    "        # Add clipping to prevent exponential overflow\n",
    "        eta = np.clip(eta, -20, 20)  # Safe range for exp()\n",
    "        mu = np.exp(eta) + epsilon  # Add epsilon for numerical stability\n",
    "        \n",
    "        # Compute gradient \n",
    "        residual = y - mu\n",
    "        # Checking for and handle NaN/Inf values\n",
    "        residual = np.nan_to_num(residual, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        gradient = -X.T @ residual / n_samples\n",
    "        \n",
    "        # Adding regularization gradients\n",
    "        if total_penalty > 0:\n",
    "            # L2 penalty component with safeguards\n",
    "            l2_grad = total_penalty * (1 - l1_ratio) * beta\n",
    "            # L1 penalty component (subgradient)\n",
    "            l1_grad = total_penalty * l1_ratio * np.sign(beta)\n",
    "            # Don't regularize intercept if present\n",
    "            if fit_intercept:\n",
    "                l2_grad[0] = 0\n",
    "                l1_grad[0] = 0\n",
    "            gradient += l2_grad + l1_grad\n",
    "        \n",
    "        # Adding gradient clipping to prevent extreme updates\n",
    "        max_grad_norm = 10.0\n",
    "        grad_norm = np.linalg.norm(gradient)\n",
    "        if grad_norm > max_grad_norm:\n",
    "            gradient = gradient * (max_grad_norm / grad_norm)\n",
    "        \n",
    "        # Calculating current learning rate using the selected schedule\n",
    "        learning_rate = get_learning_rate(iteration)\n",
    "        lr_history.append(learning_rate)\n",
    "        \n",
    "        # Updating coefficients with the current learning rate\n",
    "        beta -= learning_rate * gradient\n",
    "        \n",
    "        # Computing loss with safety checks\n",
    "        log_likelihood = np.sum(y * eta - mu)\n",
    "        reg_penalty = 0\n",
    "        if total_penalty > 0:\n",
    "            # Clip beta to prevent overflow in beta**2\n",
    "            beta_clipped = np.clip(beta, -20, 20)\n",
    "            l2_penalty = 0.5 * total_penalty * (1 - l1_ratio) * np.sum(beta_clipped**2)\n",
    "            l1_penalty = total_penalty * l1_ratio * np.sum(np.abs(beta))\n",
    "            reg_penalty = l2_penalty + l1_penalty\n",
    "        loss = -log_likelihood + reg_penalty\n",
    "        \n",
    "        # Check for invalid loss\n",
    "        if not np.isfinite(loss):\n",
    "            if verbose:\n",
    "                print(f\"Non-finite loss detected at iteration {iteration}, resetting to previous beta\")\n",
    "            beta = old_beta\n",
    "            break\n",
    "            \n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Storing iteration information\n",
    "        if return_iters:\n",
    "            beta_history.append(beta.copy())\n",
    "            nonzero_history.append(np.sum(np.abs(beta) > 1e-6))\n",
    "        \n",
    "        # Checking convergence\n",
    "        beta_change = np.linalg.norm(beta - old_beta)\n",
    "        if beta_change < tol:\n",
    "            if verbose:\n",
    "                print(f\"Converged after {iteration+1} iterations\")\n",
    "            break\n",
    "        \n",
    "        # Print progress if verbose\n",
    "        if verbose and iteration % 100 == 0:\n",
    "            print(f\"Iter {iteration}: Loss = {loss:.4f}, LR = {learning_rate:.6f}, \"\n",
    "                  f\"Non-zero = {np.sum(np.abs(beta) > 1e-6)}\")\n",
    "    \n",
    "    runtime = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Training completed in {runtime:.4f} seconds\")\n",
    "        print(f\"Final loss: {loss_history[-1]:.4f}\")\n",
    "        print(f\"Final learning rate: {lr_history[-1]:.6f}\")\n",
    "        print(f\"Non-zero coefficients: {np.sum(np.abs(beta) > 1e-6)}\")\n",
    "    \n",
    "    # Tracking non-zero coefficients for the final model\n",
    "    if not nonzero_history:\n",
    "        nonzero_history = [np.sum(np.abs(beta) > 1e-6)]\n",
    "    \n",
    "    # For consistency with other optimizers\n",
    "    if return_iters:\n",
    "        if not beta_history:\n",
    "            beta_history = [beta.copy()]\n",
    "        # Add learning rate history to the return tuple\n",
    "        return beta, loss_history, runtime, nonzero_history, beta_history, lr_history\n",
    "    else:\n",
    "        return beta, loss_history, runtime, nonzero_history\n",
    "\n",
    "\n",
    "def evaluate_lr_schedules(X, y, schedules=True, alphas=None, max_iter=500, verbose=True):\n",
    "    \"\"\"\n",
    "    Compare different learning rate schedules for GLMnet optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Feature matrix.\n",
    "    y : array-like, shape (n_samples,)\n",
    "        Target variable (count data).\n",
    "    schedules : list or None\n",
    "        List of learning rate schedules to evaluate.\n",
    "    alphas : list or None\n",
    "        List of regularization strengths to evaluate.\n",
    "    max_iter : int\n",
    "        Maximum number of iterations for each run.\n",
    "    verbose : bool\n",
    "        Whether to print results.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Results for each schedule and alpha.\n",
    "    \"\"\"\n",
    "    if schedules is None:\n",
    "        schedules = ['constant', 'inverse_time', 'exponential', 'step']\n",
    "    \n",
    "    if alphas is None:\n",
    "        alphas = [0.001, 0.01, 0.1, 1.0]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for schedule in schedules:\n",
    "        schedule_results = {}\n",
    "        \n",
    "        for alpha in alphas:\n",
    "            if verbose:\n",
    "                print(f\"\\nEvaluating {schedule} schedule with alpha={alpha}\")\n",
    "            \n",
    "            # Run GLMnet with the current schedule and alpha\n",
    "            beta, losses, runtime, nonzeros, betas, lrs = glmnet(\n",
    "                X, y,\n",
    "                alpha=alpha,\n",
    "                penalty='elasticnet',\n",
    "                max_iter=max_iter,\n",
    "                verbose=verbose,\n",
    "                lr_schedule=schedule,\n",
    "                return_iters=True\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            schedule_results[alpha] = {\n",
    "                'beta': beta,\n",
    "                'final_loss': losses[-1],\n",
    "                'iterations': len(losses),\n",
    "                'runtime': runtime,\n",
    "                'nonzero_coefs': nonzeros[-1],\n",
    "                'loss_history': losses,\n",
    "                'lr_history': lrs\n",
    "            }\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Final loss: {losses[-1]:.6f}\")\n",
    "                print(f\"  Iterations: {len(losses)}\")\n",
    "                print(f\"  Runtime: {runtime:.4f} seconds\")\n",
    "                print(f\"  Non-zero coefficients: {nonzeros[-1]}\")\n",
    "        \n",
    "        results[schedule] = schedule_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess the ecological health dataset\n",
    "    \n",
    "def preprocess_ecological_dataset(filepath=\"ecological_health_dataset.csv\"):\n",
    "    \"\"\"\n",
    "    Load and preprocess the ecological health dataset\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the dataset CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy.ndarray\n",
    "        Preprocessed feature matrix\n",
    "    y : numpy.ndarray\n",
    "        Biodiversity_Index target variable\n",
    "    feature_names : list\n",
    "        Names of the features after preprocessing\n",
    "    \"\"\"\n",
    "    print(\"Loading and preprocessing the ecological health dataset...\")\n",
    "    \n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Displaying basic information\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Target variable distribution:\\n{df['Biodiversity_Index'].value_counts().sort_index().head()}\")\n",
    "    \n",
    "    # Remove  timestamp column if it exists\n",
    "    if 'Timestamp' in df.columns:\n",
    "        df = df.drop(columns=['Timestamp'])\n",
    "    \n",
    "    # Checking for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"Missing values detected. Filling with appropriate values...\")\n",
    "        # Filling numeric columns with median\n",
    "        numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        \n",
    "        # Filling categorical columns with mode\n",
    "        categorical_cols = ['Pollution_Level', 'Ecological_Health_Label']\n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    \n",
    "    # Identifying categorical columns for encoding\n",
    "    categorical_cols = [col for col in ['Pollution_Level', 'Ecological_Health_Label'] if col in df.columns]\n",
    "    \n",
    "    # Creating preprocessor with column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), \n",
    "             [col for col in df.columns if col not in categorical_cols + ['Biodiversity_Index']]),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    # Extracting features and target\n",
    "    X = df.drop(columns=['Biodiversity_Index'])\n",
    "    y = df['Biodiversity_Index'].values\n",
    "    \n",
    "    # Fitting and transforming the features\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # feature names after preprocessing\n",
    "    numeric_cols = [col for col in df.columns if col not in categorical_cols + ['Biodiversity_Index']]\n",
    "    \n",
    "    #  one-hot encoded feature names\n",
    "    ohe = preprocessor.named_transformers_['cat']\n",
    "    cat_features = []\n",
    "    for i, col in enumerate(categorical_cols):\n",
    "        categories = ohe.categories_[i][1:]  \n",
    "        cat_features.extend([f\"{col}_{cat}\" for cat in categories])\n",
    "    \n",
    "    feature_names = numeric_cols + cat_features\n",
    "    \n",
    "    print(f\"Processed features shape: {X_processed.shape}\")\n",
    "    print(f\"Target variable shape: {y.shape}\")\n",
    "    \n",
    "    return X_processed, y, feature_names\n",
    "    \n",
    "## Cross validation\n",
    "\n",
    "def k_fold_cross_validation(X_val, y_val, k=5, lambda_values=None, seed=42, optimizers_to_use=None):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation to find optimal parameters for selected optimizers\n",
    "    and regularization types\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_val : numpy.ndarray\n",
    "        Validation feature matrix\n",
    "    y_val : numpy.ndarray\n",
    "        Validation target values\n",
    "    k : int\n",
    "        Number of folds for cross-validation\n",
    "    lambda_values : list\n",
    "        List of lambda values to try\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    optimizers_to_use : list or None\n",
    "        List of optimizer names to evaluate. If None, use all available optimizers.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_params : dict\n",
    "        Dictionary with best parameters for each optimizer and metric\n",
    "    cv_results : pd.DataFrame\n",
    "        DataFrame with all cross-validation results\n",
    "    \"\"\"\n",
    "    if lambda_values is None:\n",
    "        #lambda_values = list(np.logspace(-4, 1, 50))\n",
    "        lambda_values = np.logspace(-4, np.log10(20), 50)\n",
    "\n",
    "    # Defining all available optimizers with their base configurations\n",
    "    all_optimizers = {\n",
    "        \"AMGD\": {\n",
    "            \"func\": amgd,\n",
    "            \"base_params\": {\"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \"T\": 20.0, \n",
    "                          \"tol\": 1e-6, \"max_iter\": 1000, \"eta\": 0.0001, \"epsilon\": 1e-8}\n",
    "        },\n",
    "        \"Adam\": {\n",
    "            \"func\": adam,\n",
    "            \"base_params\": {\"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \n",
    "                          \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8}\n",
    "        },\n",
    "        \"AdaGrad\": {\n",
    "            \"func\": adagrad,\n",
    "            \"base_params\": {\"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8}\n",
    "        },\n",
    "        \"GLMnet\": {\n",
    "            \"func\": glmnet,\n",
    "            \"base_params\": {\"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Filter optimizers\n",
    "    if optimizers_to_use is not None:\n",
    "        optimizers = {name: config for name, config in all_optimizers.items() \n",
    "                     if name in optimizers_to_use}\n",
    "    else:\n",
    "        optimizers = all_optimizers\n",
    "    \n",
    "    # Regularization types\n",
    "    regularizations = [\"L1\", \"ElasticNet\"]\n",
    "    \n",
    "    # Set up k-fold cross-validation\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Store results\n",
    "    cv_results = []\n",
    "    \n",
    "    # Loop through all combinations\n",
    "    for optimizer_name, optimizer_info in optimizers.items():\n",
    "        for reg_type in regularizations:\n",
    "            for lambda_val in lambda_values:\n",
    "                print(f\"Evaluating {optimizer_name} with {reg_type} regularization, lambda={lambda_val}\")\n",
    "                \n",
    "                # Prepare parameters based on regularization type\n",
    "                params = optimizer_info[\"base_params\"].copy()\n",
    "                \n",
    "                if reg_type == \"L1\":\n",
    "                    params[\"lambda1\"] = lambda_val\n",
    "                    params[\"lambda2\"] = 0.0\n",
    "                    params[\"penalty\"] = \"l1\"\n",
    "                elif reg_type == \"ElasticNet\":\n",
    "                    params[\"lambda1\"] = lambda_val / 2  # Distribute lambda between L1 and L2\n",
    "                    params[\"lambda2\"] = lambda_val / 2\n",
    "                    params[\"penalty\"] = \"elasticnet\"\n",
    "                \n",
    "                # Metrics for this combination across all folds\n",
    "                fold_maes = []\n",
    "                fold_rmses = []\n",
    "                fold_deviances = []\n",
    "                fold_runtimes = []\n",
    "                fold_sparsities = []\n",
    "                \n",
    "                # Run k-fold cross-validation\n",
    "                for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X_val)):\n",
    "                    X_fold_train, X_fold_test = X_val[train_idx], X_val[test_idx]\n",
    "                    y_fold_train, y_fold_test = y_val[train_idx], y_val[test_idx]\n",
    "                    \n",
    "                    # Train the model on this fold - handle different return value counts\n",
    "                    try:\n",
    "                        # gett all 5 return values\n",
    "                        beta, loss_history, runtime, nonzero_history, _ = optimizer_info[\"func\"](\n",
    "                            X_fold_train, y_fold_train, **params, verbose=False, return_iters=False\n",
    "                        )\n",
    "                    except ValueError:\n",
    "                        \n",
    "                        beta, loss_history, runtime, nonzero_history = optimizer_info[\"func\"](\n",
    "                            X_fold_train, y_fold_train, **params, verbose=False, return_iters=False\n",
    "                        )\n",
    "                    \n",
    "                    # Evaluating on the hold-out fold\n",
    "                    metrics = evaluate_model(beta, X_fold_test, y_fold_test)\n",
    "                    \n",
    "                    # Store metrics\n",
    "                    fold_maes.append(metrics['MAE'])\n",
    "                    fold_rmses.append(metrics['RMSE'])\n",
    "                    fold_deviances.append(metrics['Mean Deviance'])\n",
    "                    fold_runtimes.append(runtime)\n",
    "                    fold_sparsities.append(metrics['Sparsity'])\n",
    "                \n",
    "                # Calculating average metrics across folds\n",
    "                avg_mae = np.mean(fold_maes)\n",
    "                avg_rmse = np.mean(fold_rmses)\n",
    "                avg_deviance = np.mean(fold_deviances)\n",
    "                avg_runtime = np.mean(fold_runtimes)\n",
    "                avg_sparsity = np.mean(fold_sparsities)\n",
    "                \n",
    "                # Calculating standard deviations\n",
    "                std_mae = np.std(fold_maes)\n",
    "                std_rmse = np.std(fold_rmses)\n",
    "                std_deviance = np.std(fold_deviances)\n",
    "                \n",
    "                # Storing the results\n",
    "                result = {\n",
    "                    \"Optimizer\": optimizer_name,\n",
    "                    \"Regularization\": reg_type,\n",
    "                    \"Lambda\": lambda_val,\n",
    "                    \"MAE\": avg_mae,\n",
    "                    \"MAE_std\": std_mae,\n",
    "                    \"RMSE\": avg_rmse,\n",
    "                    \"RMSE_std\": std_rmse,\n",
    "                    \"Mean Deviance\": avg_deviance,\n",
    "                    \"Mean Deviance_std\": std_deviance,\n",
    "                    \"Runtime\": avg_runtime,\n",
    "                    \"Sparsity\": avg_sparsity\n",
    "                }\n",
    "                \n",
    "                cv_results.append(result)\n",
    "    \n",
    "    # Converting results to DataFrame\n",
    "    cv_results_df = pd.DataFrame(cv_results)\n",
    "    \n",
    "    # Finding best parameters for each metric\n",
    "    best_params = {}\n",
    "    \n",
    "    for optimizer_name in optimizers.keys():\n",
    "        optimizer_results = cv_results_df[cv_results_df['Optimizer'] == optimizer_name]\n",
    "        \n",
    "        # Finding best parameters for MAE\n",
    "        best_mae_idx = optimizer_results['MAE'].idxmin()\n",
    "        best_params[f\"{optimizer_name}_MAE\"] = {\n",
    "            \"Optimizer\": optimizer_name,\n",
    "            \"Regularization\": optimizer_results.loc[best_mae_idx, 'Regularization'],\n",
    "            \"Lambda\": optimizer_results.loc[best_mae_idx, 'Lambda'],\n",
    "            \"Metric_Value\": optimizer_results.loc[best_mae_idx, 'MAE']\n",
    "        }\n",
    "        \n",
    "        # Finding best parameters for RMSE\n",
    "        best_rmse_idx = optimizer_results['RMSE'].idxmin()\n",
    "        best_params[f\"{optimizer_name}_RMSE\"] = {\n",
    "            \"Optimizer\": optimizer_name,\n",
    "            \"Regularization\": optimizer_results.loc[best_rmse_idx, 'Regularization'],\n",
    "            \"Lambda\": optimizer_results.loc[best_rmse_idx, 'Lambda'],\n",
    "            \"Metric_Value\": optimizer_results.loc[best_rmse_idx, 'RMSE']\n",
    "        }\n",
    "        \n",
    "        # Finding best parameters for Mean Deviance\n",
    "        best_dev_idx = optimizer_results['Mean Deviance'].idxmin()\n",
    "        best_params[f\"{optimizer_name}_Mean_Deviance\"] = {\n",
    "            \"Optimizer\": optimizer_name,\n",
    "            \"Regularization\": optimizer_results.loc[best_dev_idx, 'Regularization'],\n",
    "            \"Lambda\": optimizer_results.loc[best_dev_idx, 'Lambda'],\n",
    "            \"Metric_Value\": optimizer_results.loc[best_dev_idx, 'Mean Deviance']\n",
    "        }\n",
    "        \n",
    "        # Finding best parameters for Runtime (fastest)\n",
    "        best_runtime_idx = optimizer_results['Runtime'].idxmin()\n",
    "        best_params[f\"{optimizer_name}_Runtime\"] = {\n",
    "            \"Optimizer\": optimizer_name,\n",
    "            \"Regularization\": optimizer_results.loc[best_runtime_idx, 'Regularization'],\n",
    "            \"Lambda\": optimizer_results.loc[best_runtime_idx, 'Lambda'],\n",
    "            \"Metric_Value\": optimizer_results.loc[best_runtime_idx, 'Runtime']\n",
    "        }\n",
    "        \n",
    "        # Finding best parameters for Sparsity\n",
    "        best_sparsity_idx = optimizer_results['Sparsity'].idxmax()\n",
    "        best_params[f\"{optimizer_name}_Sparsity\"] = {\n",
    "            \"Optimizer\": optimizer_name,\n",
    "            \"Regularization\": optimizer_results.loc[best_sparsity_idx, 'Regularization'],\n",
    "            \"Lambda\": optimizer_results.loc[best_sparsity_idx, 'Lambda'],\n",
    "            \"Metric_Value\": optimizer_results.loc[best_sparsity_idx, 'Sparsity']\n",
    "        }\n",
    "    \n",
    "    # Finding the overall best parameters\n",
    "    mae_results = [params for key, params in best_params.items() if key.endswith('_MAE')]\n",
    "    best_mae_params = min(mae_results, key=lambda x: x['Metric_Value'])\n",
    "    best_params['Overall_Best_MAE'] = best_mae_params\n",
    "    \n",
    "    rmse_results = [params for key, params in best_params.items() if key.endswith('_RMSE')]\n",
    "    best_rmse_params = min(rmse_results, key=lambda x: x['Metric_Value'])\n",
    "    best_params['Overall_Best_RMSE'] = best_rmse_params\n",
    "    \n",
    "    deviance_results = [params for key, params in best_params.items() if key.endswith('_Mean_Deviance')]\n",
    "    best_deviance_params = min(deviance_results, key=lambda x: x['Metric_Value'])\n",
    "    best_params['Overall_Best_Mean_Deviance'] = best_deviance_params\n",
    "    \n",
    "    runtime_results = [params for key, params in best_params.items() if key.endswith('_Runtime')]\n",
    "    best_runtime_params = min(runtime_results, key=lambda x: x['Metric_Value'])\n",
    "    best_params['Overall_Best_Runtime'] = best_runtime_params\n",
    "    \n",
    "    sparsity_results = [params for key, params in best_params.items() if key.endswith('_Sparsity')]\n",
    "    best_sparsity_params = max(sparsity_results, key=lambda x: x['Metric_Value'])\n",
    "    best_params['Overall_Best_Sparsity'] = best_sparsity_params\n",
    "    \n",
    "    # Printing a summary of best lambda values for each optimizer and metric\n",
    "    print(\"\\nSummary of best lambda values from cross-validation:\")\n",
    "    print(\"{:<15} {:<15} {:<15} {:<15} {:<15}\".format(\"Optimizer\", \"MAE Lambda\", \"RMSE Lambda\", \"Deviance Lambda\", \"Sparsity Lambda\"))\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for optimizer in optimizers.keys():\n",
    "        mae_lambda = best_params[f\"{optimizer}_MAE\"][\"Lambda\"]\n",
    "        rmse_lambda = best_params[f\"{optimizer}_RMSE\"][\"Lambda\"]\n",
    "        dev_lambda = best_params[f\"{optimizer}_Mean_Deviance\"][\"Lambda\"]\n",
    "        spar_lambda = best_params[f\"{optimizer}_Sparsity\"][\"Lambda\"]\n",
    "        \n",
    "        print(\"{:<15} {:<15.6f} {:<15.6f} {:<15.6f} {:<15.6f}\".format(\n",
    "            optimizer, mae_lambda, rmse_lambda, dev_lambda, spar_lambda))\n",
    "    \n",
    "    print(\"\\nOverall best parameters:\")\n",
    "    for metric in [\"MAE\", \"RMSE\", \"Mean_Deviance\", \"Runtime\", \"Sparsity\"]:\n",
    "        params = best_params[f\"Overall_Best_{metric}\"]\n",
    "        print(f\"Best for {metric}: {params['Optimizer']} with {params['Regularization']} (λ={params['Lambda']:.6f})\")\n",
    "    \n",
    "    return best_params, cv_results_df\n",
    "\n",
    "\n",
    "#Function to train all optimizers\n",
    "\n",
    "def train_all_optimizers(X_train, y_train, best_params, metric='MAE'):\n",
    "    \"\"\"\n",
    "    Train all optimizer models using their best parameters from cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy.ndarray\n",
    "        Training feature matrix\n",
    "    y_train : numpy.ndarray\n",
    "        Training target values\n",
    "    best_params : dict\n",
    "        Dictionary with best parameters for each optimizer and metric\n",
    "    metric : str\n",
    "        Metric to use for selecting the best parameters ('MAE', 'RMSE', 'Mean_Deviance', 'Runtime', 'Sparsity')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model_results : dict\n",
    "        Dictionary with trained models, their coefficients, and performance metrics\n",
    "    \"\"\"\n",
    "    optimizers = ['AMGD', 'Adam', 'AdaGrad', 'GLMnet']\n",
    "    model_results = {}\n",
    "    \n",
    "    for optimizer_name in optimizers:\n",
    "        # Get best parameters for this optimizer and metric\n",
    "        params = best_params[f\"{optimizer_name}_{metric}\"]\n",
    "        \n",
    "        reg_type = params['Regularization']\n",
    "        lambda_val = params['Lambda']\n",
    "        \n",
    "        print(f\"\\nTraining {optimizer_name} with best parameters for {metric}:\")\n",
    "        print(f\"  Regularization: {reg_type}\")\n",
    "        print(f\"  Lambda: {lambda_val}\")\n",
    "        \n",
    "        # Setting uptimizer function and parameters\n",
    "        if optimizer_name == \"AMGD\":\n",
    "            optimizer_func = amgd\n",
    "            base_params = {\"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \"T\": 20.0, \n",
    "                          \"tol\": 1e-6, \"max_iter\": 1000, \"eta\": 0.0001, \"epsilon\": 1e-8}\n",
    "        elif optimizer_name == \"Adam\":\n",
    "            optimizer_func = adam\n",
    "            base_params = {\"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \n",
    "                          \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8}\n",
    "        elif optimizer_name == \"AdaGrad\":\n",
    "            optimizer_func = adagrad\n",
    "            base_params = {\"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8}\n",
    "        else:  # GLMnet\n",
    "            optimizer_func = glmnet\n",
    "            base_params = {\"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8}\n",
    "        \n",
    "        # Configure regularization parameters\n",
    "        if reg_type == \"L1\":\n",
    "            base_params[\"lambda1\"] = lambda_val\n",
    "            base_params[\"lambda2\"] = 0.0\n",
    "            base_params[\"penalty\"] = \"l1\"\n",
    "        else:  # ElasticNet\n",
    "            base_params[\"lambda1\"] = lambda_val / 2\n",
    "            base_params[\"lambda2\"] = lambda_val / 2\n",
    "            base_params[\"penalty\"] = \"elasticnet\"\n",
    "        \n",
    "        # Train the model with tracking of per-iteration values\n",
    "        try:\n",
    "            # Try for GLMnet which returns 6 values when return_iters=True\n",
    "            if optimizer_name == \"GLMnet\":\n",
    "                beta, loss_history, runtime, nonzero_history, beta_history, lr_history = optimizer_func(\n",
    "                    X_train, y_train, **base_params, verbose=True, return_iters=True\n",
    "                )\n",
    "            else:\n",
    "                # For AMGD, Adam, and AdaGrad which return 5 values\n",
    "                beta, loss_history, runtime, nonzero_history, beta_history = optimizer_func(\n",
    "                    X_train, y_train, **base_params, verbose=True, return_iters=True\n",
    "                )\n",
    "                lr_history = None  # These optimizers don't return learning rate\n",
    "        except ValueError as e:\n",
    "            print(f\"Error with {optimizer_name}: {e}\")\n",
    "            print(\"Trying alternative return value handling...\")\n",
    "            \n",
    "            # Fallback approach\n",
    "            result = optimizer_func(X_train, y_train, **base_params, verbose=True, return_iters=True)\n",
    "            \n",
    "            # Extracting values based on the length of the result tuple\n",
    "            if len(result) == 6:  # GLMnet with lr_history\n",
    "                beta, loss_history, runtime, nonzero_history, beta_history, lr_history = result\n",
    "            elif len(result) == 5:  # AMGD, Adam, AdaGrad\n",
    "                beta, loss_history, runtime, nonzero_history, beta_history = result\n",
    "                lr_history = None\n",
    "            else:\n",
    "                # Handle any other unexpected return value count\n",
    "                beta = result[0]\n",
    "                loss_history = result[1] if len(result) > 1 else []\n",
    "                runtime = result[2] if len(result) > 2 else 0\n",
    "                nonzero_history = result[3] if len(result) > 3 else []\n",
    "                beta_history = result[4] if len(result) > 4 else []\n",
    "                lr_history = result[5] if len(result) > 5 else None\n",
    "        \n",
    "        # Evaluating model on training set\n",
    "        train_metrics = evaluate_model(beta, X_train, y_train)\n",
    "        \n",
    "        # Storing results\n",
    "        model_results[optimizer_name] = {\n",
    "            'beta': beta,\n",
    "            'loss_history': loss_history,\n",
    "            'runtime': runtime,\n",
    "            'nonzero_history': nonzero_history,\n",
    "            'beta_history': beta_history,\n",
    "            'lr_history': lr_history,\n",
    "            'train_metrics': train_metrics,\n",
    "            'params': base_params\n",
    "        }\n",
    "        \n",
    "        print(f\"  Training complete in {runtime:.2f} seconds\")\n",
    "        print(f\"  Training MAE: {train_metrics['MAE']:.4f}\")\n",
    "        print(f\"  Training RMSE: {train_metrics['RMSE']:.4f}\")\n",
    "        print(f\"  Training Mean Deviance: {train_metrics['Mean Deviance']:.4f}\")\n",
    "        print(f\"  Sparsity: {train_metrics['Sparsity']:.4f}\")\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "\n",
    "def create_algorithm_comparison_plots(cv_results_df):\n",
    "    \"\"\"\n",
    "    Create barplots comparing the performance of AMGD, AdaGrad, Adam, and GLMnet algorithms\n",
    "    across different metrics with L1 and ElasticNet regularization only.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cv_results_df : pandas.DataFrame\n",
    "        DataFrame with cross-validation results\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    figs : list\n",
    "        List of matplotlib figures\n",
    "    \"\"\"\n",
    "    # Create a list to store the figures\n",
    "    figs = []\n",
    "    \n",
    "    # Metrics to compare\n",
    "    metrics = ['MAE', 'RMSE', 'Mean Deviance', 'Runtime']\n",
    "    \n",
    "    #Best result for each optimizer and metric combination\n",
    "    best_results = []\n",
    "    \n",
    "    for optimizer in ['AMGD', 'AdaGrad', 'Adam', 'GLMnet']:\n",
    "        optimizer_df = cv_results_df[cv_results_df['Optimizer'] == optimizer]\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric in ['MAE', 'RMSE', 'Mean Deviance']:\n",
    "                # For these metrics, lower is better\n",
    "                best_idx = optimizer_df[metric].idxmin()\n",
    "            else:  # Runtime\n",
    "                # For runtime, lower is better\n",
    "                best_idx = optimizer_df['Runtime'].idxmin()\n",
    "            \n",
    "            best_results.append({\n",
    "                'Optimizer': optimizer,\n",
    "                'Metric': metric,\n",
    "                'Value': optimizer_df.loc[best_idx, metric],\n",
    "                'Regularization': optimizer_df.loc[best_idx, 'Regularization'],\n",
    "                'Lambda': optimizer_df.loc[best_idx, 'Lambda']\n",
    "            })\n",
    "    \n",
    "    # Converting to DataFrame \n",
    "    best_results_df = pd.DataFrame(best_results)\n",
    "    \n",
    "    # Creating a barplot for each metric\n",
    "    for metric in metrics:\n",
    "        metric_df = best_results_df[best_results_df['Metric'] == metric]\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 7))\n",
    "        \n",
    "        # Colors for each optimizer\n",
    "        colors = {'AMGD': '#3498db', 'AdaGrad': '#2ecc71', 'Adam': '#e74c3c', 'GLMnet': '#9b59b6'}\n",
    "        bar_colors = [colors[opt] for opt in metric_df['Optimizer']]\n",
    "        \n",
    "        # Creating barplot\n",
    "        bars = ax.bar(metric_df['Optimizer'], metric_df['Value'], color=bar_colors)\n",
    "        \n",
    "        # Adding value labels on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + (height * 0.02),\n",
    "                   f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # Adding regularization and lambda information below bars\n",
    "        for i, (_, row) in enumerate(metric_df.iterrows()):\n",
    "            ax.text(i, 0, f\"{row['Regularization']}\\nλ={row['Lambda']:.4f}\", \n",
    "                   ha='center', va='bottom', fontsize=8, color='black',\n",
    "                   transform=ax.get_xaxis_transform())\n",
    "        \n",
    "        # Set title and labels\n",
    "        ax.set_title(f'Best {metric} Comparison Across Optimizers (L1/ElasticNet)', fontsize=14)\n",
    "        ax.set_ylabel(metric, fontsize=14)\n",
    "        ax.set_xlabel('Optimizer', fontsize=14)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        \n",
    "        plt.ylim(0, metric_df['Value'].max() * 1.15)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        figs.append(fig)\n",
    "    \n",
    "    return figs\n",
    "\n",
    "# Function to compare convergence rates\n",
    "def compare_convergence_rates(X_train, y_train, best_params):\n",
    "    \"\"\"\n",
    "    Compare convergence rates of optimization algorithms\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy.ndarray\n",
    "        Training feature matrix\n",
    "    y_train : numpy.ndarray\n",
    "        Training target values\n",
    "    best_params : dict\n",
    "        Dictionary with best parameters for each optimizer\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.figure.Figure\n",
    "        Plot comparing convergence rates of all optimizers\n",
    "    \"\"\"\n",
    "    print(\"Comparing convergence rates of optimization algorithms...\")\n",
    "    \n",
    "    # Include all optimizers\n",
    "    optimizers = ['AMGD', 'Adam', 'AdaGrad', 'GLMnet']\n",
    "    optimizer_functions = {\n",
    "        'AMGD': amgd, \n",
    "        'Adam': adam, \n",
    "        'AdaGrad': adagrad, \n",
    "        'GLMnet': glmnet\n",
    "    }\n",
    "    colors = {\n",
    "        'AMGD': '#3498db',  # Blue\n",
    "        'Adam': '#e74c3c',  # Red\n",
    "        'AdaGrad': '#2ecc71',  # Green\n",
    "        'GLMnet': '#9b59b6'  # Purple\n",
    "    }\n",
    "    linestyles = {\n",
    "        'AMGD': '-', \n",
    "        'Adam': '--', \n",
    "        'AdaGrad': '-.', \n",
    "        'GLMnet': ':'\n",
    "    }\n",
    "    \n",
    "    # Store loss histories\n",
    "    all_loss_histories = {}\n",
    "    \n",
    "    for optimizer_name in optimizers:\n",
    "        # Get best parameters for MAE (or RMSE)\n",
    "        params = best_params[f'{optimizer_name}_MAE']\n",
    "        reg_type = params['Regularization']\n",
    "        lambda_val = params['Lambda']\n",
    "        \n",
    "        # Setup base parameters\n",
    "        if optimizer_name == \"AMGD\":\n",
    "            base_params = {\n",
    "                \"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \"T\": 20.0, \n",
    "                \"tol\": 1e-6, \"max_iter\": 1000, \"eta\": 0.0001, \"epsilon\": 1e-8\n",
    "            }\n",
    "        elif optimizer_name == \"Adam\":\n",
    "            base_params = {\n",
    "                \"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \n",
    "                \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8\n",
    "            }\n",
    "        elif optimizer_name == 'GLMnet':\n",
    "            base_params = {\n",
    "                \"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 1000, \n",
    "                \"epsilon\": 1e-8, \"is_pre_scaled\": True\n",
    "            }\n",
    "        else:  # AdaGrad\n",
    "            base_params = {\n",
    "                \"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8\n",
    "            }\n",
    "        \n",
    "        # Configure regularization parameters\n",
    "        if reg_type == \"L1\":\n",
    "            base_params[\"lambda1\"] = lambda_val\n",
    "            base_params[\"lambda2\"] = 0.0\n",
    "            base_params[\"penalty\"] = \"l1\"\n",
    "        else:  # ElasticNet\n",
    "            base_params[\"lambda1\"] = lambda_val / 2\n",
    "            base_params[\"lambda2\"] = lambda_val / 2\n",
    "            base_params[\"penalty\"] = \"elasticnet\"\n",
    "        \n",
    "        # Run optimizer and track loss history\n",
    "        _, loss_history, _, _ = optimizer_functions[optimizer_name](\n",
    "            X_train, y_train, **base_params, verbose=False, return_iters=False\n",
    "        )\n",
    "        \n",
    "        # Only store non-empty loss histories\n",
    "        if len(loss_history) > 0:\n",
    "            all_loss_histories[optimizer_name] = loss_history\n",
    "        else:\n",
    "            print(f\"Warning: {optimizer_name} returned an empty loss history. Skipping in convergence plot.\")\n",
    "    \n",
    "    # Check if we have any valid loss histories to plot\n",
    "    if not all_loss_histories:\n",
    "        print(\"Error: No valid loss histories to plot. Check that at least one optimizer returns non-empty loss history.\")\n",
    "        # Return an empty figure \n",
    "        return plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Create comprehensive convergence plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Main convergence plot (log scale)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    for optimizer_name, loss_history in all_loss_histories.items():\n",
    "        # Calculate percentage of max iterations (normalization)\n",
    "        iterations = np.linspace(0, 100, len(loss_history))\n",
    "        \n",
    "        # Plot with log scale for y-axis\n",
    "        plt.semilogy(\n",
    "            iterations, \n",
    "            loss_history, \n",
    "            label=optimizer_name, \n",
    "            color=colors[optimizer_name], \n",
    "            linestyle=linestyles[optimizer_name], \n",
    "            linewidth=2\n",
    "        )\n",
    "    \n",
    "    plt.title('Convergence Rate Comparison (Log Scale)', fontsize=14)\n",
    "    plt.xlabel('Percentage of Max Iterations (%)', fontsize=13)\n",
    "    plt.ylabel('Loss (log scale)', fontsize=13)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(title='Optimizer', loc='best')\n",
    "    \n",
    "    # Normalized convergence plot (if we have multiple valid histories)\n",
    "    if len(all_loss_histories) > 1:\n",
    "        plt.subplot(2, 1, 2)\n",
    "        max_lengths = max(len(loss_history) for loss_history in all_loss_histories.values())\n",
    "        \n",
    "        for optimizer_name, loss_history in all_loss_histories.items():\n",
    "            # Skip empty loss histories\n",
    "            if len(loss_history) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Normalize loss history to same length\n",
    "            if len(loss_history) < max_lengths:\n",
    "                # Interpolate to match max length\n",
    "                try:\n",
    "                    x_new = np.linspace(0, 1, max_lengths)\n",
    "                    x_old = np.linspace(0, 1, len(loss_history))\n",
    "                    normalized_loss = np.interp(x_new, x_old, loss_history)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error interpolating {optimizer_name} loss history: {e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                normalized_loss = loss_history\n",
    "            \n",
    "            plt.plot(\n",
    "                np.linspace(0, 100, len(normalized_loss)), \n",
    "                normalized_loss, \n",
    "                label=optimizer_name, \n",
    "                color=colors[optimizer_name], \n",
    "                linestyle=linestyles[optimizer_name], \n",
    "                linewidth=2\n",
    "            )\n",
    "        \n",
    "        plt.title('Normalized Convergence Rate Comparison', fontsize=14)\n",
    "        plt.xlabel('Percentage of Max Iterations (%)', fontsize=13)\n",
    "        plt.ylabel('Normalized Loss', fontsize=13)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend(title='Optimizer', loc='best')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "\n",
    "def plot_coefficient_paths_for_ecological_data():\n",
    "    \"\"\"\n",
    "    Plot coefficient paths for different optimizers using the ecological dataset\n",
    "    \"\"\"\n",
    "    print(\"Analyzing coefficient paths for ecological dataset...\")\n",
    "    \n",
    "    # 1. Load and preprocess the data\n",
    "    X, y, feature_names = preprocess_ecological_dataset(\"ecological_health_dataset.csv\")\n",
    "    \n",
    "    # 2. Split data into train, validation, and test sets (70/15/15)\n",
    "    # First split: 85% train+val, 15% test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Second split: 70% train, 15% validation (82.35% of train_val is train)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.1765, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/X.shape[0]:.1%})\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/X.shape[0]:.1%})\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/X.shape[0]:.1%})\")\n",
    "    \n",
    "    # 3. Configure lambda values for the regularization path\n",
    "    lambda_values = np.logspace(-3, 1, 10)  # From 0.001 to 10\n",
    "    \n",
    "    # 4. Select only the top most important features for readability\n",
    "    # Runing a basic model to identify important features\n",
    "    params = {\n",
    "        \"alpha\": 0.01, \n",
    "        \"beta1\": 0.9, \n",
    "        \"beta2\": 0.999, \n",
    "        \"lambda1\": 0.1,\n",
    "        \"lambda2\": 0.0,\n",
    "        \"penalty\": \"l1\",\n",
    "        \"T\": 20.0, \n",
    "        \"tol\": 1e-6, \n",
    "        \"max_iter\": 200,  # Reduced for quicker execution\n",
    "        \"eta\": 0.0001, \n",
    "        \"epsilon\": 1e-8,\n",
    "        \"verbose\": False\n",
    "    }\n",
    "    \n",
    "    initial_beta, _, _, _ = amgd(X_train, y_train, **params)\n",
    "    \n",
    "    # Finding top 10 features by coefficient magnitude\n",
    "    importance = np.abs(initial_beta)\n",
    "    top_indices = np.argsort(importance)[-17:]  # Top 10 features\n",
    "    top_feature_names = [feature_names[i] for i in top_indices]\n",
    "    \n",
    "    # 5. Creating figure for the coefficient paths\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(18, 20), sharex=True)\n",
    "    fig.suptitle('Coefficient Paths for Biodiversity Prediction: L1/ElasticNet Regularization', fontsize=16)\n",
    "    \n",
    "    # Configure plot settings\n",
    "    optimizers = ['AMGD', 'Adam', 'AdaGrad', 'GLMnet'] \n",
    "    penalty_types = ['l1', 'elasticnet']\n",
    "    colors = plt.cm.viridis(np.linspace(0, 0.8, len(top_indices)))\n",
    "    \n",
    "    # 6. Plot coefficient paths for each optimizer and regularization type\n",
    "    for i, optimizer_name in enumerate(optimizers):\n",
    "        for j, penalty in enumerate(penalty_types):\n",
    "            ax = axes[i, j]\n",
    "            \n",
    "            # Storage for coefficient values at each lambda\n",
    "            coef_paths = []\n",
    "            \n",
    "            # Running optimization for each lambda value\n",
    "            for lambda_val in lambda_values:\n",
    "                if optimizer_name == 'AMGD':\n",
    "                    params = {\n",
    "                        \"alpha\": 0.01, \n",
    "                        \"beta1\": 0.9, \n",
    "                        \"beta2\": 0.999, \n",
    "                        \"T\": 20.0, \n",
    "                        \"tol\": 1e-6, \n",
    "                        \"max_iter\": 200,  \n",
    "                        \"eta\": 0.0001, \n",
    "                        \"epsilon\": 1e-8,\n",
    "                        \"lambda1\": lambda_val if penalty == 'l1' else lambda_val/2,\n",
    "                        \"lambda2\": 0.0 if penalty == 'l1' else lambda_val/2,\n",
    "                        \"penalty\": penalty,\n",
    "                        \"verbose\": False\n",
    "                    }\n",
    "                    beta, _, _, _ = amgd(X_train, y_train, **params)\n",
    "                    \n",
    "                elif optimizer_name == 'Adam':\n",
    "                    params = {\n",
    "                        \"alpha\": 0.01, \n",
    "                        \"beta1\": 0.9, \n",
    "                        \"beta2\": 0.999, \n",
    "                        \"tol\": 1e-6, \n",
    "                        \"max_iter\": 200,  \n",
    "                        \"epsilon\": 1e-8,\n",
    "                        \"lambda1\": lambda_val if penalty == 'l1' else lambda_val/2,\n",
    "                        \"lambda2\": 0.0 if penalty == 'l1' else lambda_val/2,\n",
    "                        \"penalty\": penalty,\n",
    "                        \"verbose\": False\n",
    "                    }\n",
    "                    beta, _, _, _ = adam(X_train, y_train, **params)\n",
    "                    \n",
    "                elif optimizer_name == 'AdaGrad':\n",
    "                    params = {\n",
    "                        \"alpha\": 0.01, \n",
    "                        \"tol\": 1e-6, \n",
    "                        \"max_iter\": 200,  \n",
    "                        \"epsilon\": 1e-8,\n",
    "                        \"lambda1\": lambda_val if penalty == 'l1' else lambda_val/2,\n",
    "                        \"lambda2\": 0.0 if penalty == 'l1' else lambda_val/2,\n",
    "                        \"penalty\": penalty,\n",
    "                        \"verbose\": False\n",
    "                    }\n",
    "                    beta, _, _, _ = adagrad(X_train, y_train, **params)\n",
    "                \n",
    "                else:  # GLMnet\n",
    "                    params = {\n",
    "                        \"alpha\": 0.01,\n",
    "                        \"lambda1\": lambda_val if penalty == 'l1' else lambda_val/2,\n",
    "                        \"lambda2\": 0.0 if penalty == 'l1' else lambda_val/2,\n",
    "                        \"penalty\": penalty,\n",
    "                        \"tol\": 1e-6,\n",
    "                        \"max_iter\": 200,\n",
    "                        \"epsilon\": 1e-8,\n",
    "                        \"is_pre_scaled\": False,\n",
    "                        \"verbose\": False\n",
    "                    }\n",
    "                    try:\n",
    "                        # unpacking 5 values using only the first (beta)\n",
    "                        beta, _, _, _, _ = glmnet(X_train, y_train, **params, return_iters=True)\n",
    "                    except ValueError:\n",
    "                        # If only 4 values returned\n",
    "                        beta, _, _, _ = glmnet(X_train, y_train, **params, return_iters=False)\n",
    "                \n",
    "                # Extracting coefficients for the top features only\n",
    "                selected_coeffs = [beta[idx] for idx in top_indices]\n",
    "                coef_paths.append(selected_coeffs)\n",
    "            \n",
    "            # Converting to numpy array for easier manipulation\n",
    "            coef_paths = np.array(coef_paths)\n",
    "            \n",
    "            # Plotting coefficient paths for top features\n",
    "            for idx, feature_idx in enumerate(range(len(top_indices))):\n",
    "                ax.plot(lambda_values, coef_paths[:, idx], \n",
    "                        color=colors[idx], \n",
    "                        label=top_feature_names[idx],\n",
    "                        linewidth=2)\n",
    "            \n",
    "            # labels and title\n",
    "            ax.set_xscale('log')\n",
    "            ax.set_xlabel('Regularization Strength (λ)' if i == 3 else '')  \n",
    "            ax.set_ylabel('Coefficient Value' if j == 0 else '')\n",
    "            ax.set_title(f'{optimizer_name} - {penalty.capitalize()} Regularization')\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "            ax.axvline(x=0.1, color='gray', linestyle='--', alpha=0.5)\n",
    "            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # legend \n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 0.08),\n",
    "               title='Features', ncol=5, frameon=True)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.96])\n",
    "    plt.savefig('ecological_coefficient_paths.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    #plot_coefficient_evolution_for_ecological_data(X_train, y_train, top_indices, top_feature_names)\n",
    "\n",
    "\n",
    "def plot_training_and_test_metrics(model_results, test_metrics, metric_to_plot='MAE'):\n",
    "    \"\"\"\n",
    "    Plot training and test metrics across optimizers.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_results : dict\n",
    "        Dictionary containing all optimizer models and their results\n",
    "    test_metrics : dict\n",
    "        Dictionary containing test metrics for all optimizer models\n",
    "    metric_to_plot : str\n",
    "        Metric to plot ('MAE', 'RMSE', 'Mean Deviance')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plots)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 1. Plot loss histories (convergence) for all optimizers\n",
    "    plt.subplot(2, 2, 1)\n",
    "    \n",
    "    colors = {'AMGD': '#3498db', 'Adam': '#e74c3c', 'AdaGrad': '#2ecc71', 'GLMnet': '#9b59b6'}\n",
    "    \n",
    "    for optimizer_name, results in model_results.items():\n",
    "        loss_history = results['loss_history']\n",
    "        iterations = np.arange(1, len(loss_history) + 1)\n",
    "        plt.semilogy(iterations, loss_history, label=f\"{optimizer_name}\", color=colors[optimizer_name], linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Iterations', fontsize=13)\n",
    "    plt.ylabel('Loss (log scale)', fontsize=13)\n",
    "    plt.title('Training Loss Convergence', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=11)\n",
    "    \n",
    "    # 2. Plot training metrics for all optimizers\n",
    "    plt.subplot(2, 2, 2)\n",
    "    optimizer_names = list(model_results.keys())\n",
    "    train_metrics = [model_results[opt]['train_metrics'][metric_to_plot] for opt in optimizer_names]\n",
    "    \n",
    "    # Bar chart for training metrics\n",
    "    bars = plt.bar(optimizer_names, train_metrics, color=[colors[opt] for opt in optimizer_names])\n",
    "\n",
    "    #Value label\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + (height * 0.02),\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.ylabel(f'Training {metric_to_plot}', fontsize=12)\n",
    "    plt.title(f'Training {metric_to_plot} by Optimizer', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 3. Plotting test metrics for all optimizers\n",
    "    plt.subplot(2, 2, 3)\n",
    "    test_metric_values = [test_metrics[opt][metric_to_plot] for opt in optimizer_names]\n",
    "    \n",
    "    # Bar chart for test metrics\n",
    "    bars = plt.bar(optimizer_names, test_metric_values, color=[colors[opt] for opt in optimizer_names])\n",
    "    \n",
    "    # Value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + (height * 0.02),\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.ylabel(f'Test {metric_to_plot}', fontsize=12)\n",
    "    plt.title(f'Test {metric_to_plot} by Optimizer', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 4. Plotting train vs test comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    x = np.arange(len(optimizer_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    #Grouped bar chart\n",
    "    bars1 = plt.bar(x - width/2, train_metrics, width, label=f'Training {metric_to_plot}', alpha=0.7)\n",
    "    bars2 = plt.bar(x + width/2, test_metric_values, width, label=f'Test {metric_to_plot}', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Optimizer', fontsize=12)\n",
    "    plt.ylabel(metric_to_plot, fontsize=12)\n",
    "    plt.title(f'Training vs Test {metric_to_plot} Comparison', fontsize=14)\n",
    "    plt.xticks(x, optimizer_names)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # figure for the feature importance comparison\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    \n",
    "    # Subplots for each optimizer's feature importance \n",
    "    for i, optimizer_name in enumerate(optimizer_names, 1):\n",
    "        plt.subplot(1, len(optimizer_names), i)\n",
    "        \n",
    "        #Beta coefficients for this optimizer\n",
    "        beta = model_results[optimizer_name]['beta']\n",
    "        \n",
    "        # Calculating feature importance based on absolute coefficient values\n",
    "        importance = np.abs(beta)\n",
    "        indices = np.argsort(importance)[::-1]\n",
    "        \n",
    "        #Top N features\n",
    "        top_n = min(10, len(importance))\n",
    "        \n",
    "        # Get feature names \n",
    "        feature_indices = indices[:top_n]\n",
    "        feature_labels = [f\"Feature {idx}\" for idx in feature_indices]  # Replace with actual feature names if available\n",
    "        \n",
    "        # Plot importance\n",
    "        plt.barh(range(top_n), importance[feature_indices], align='center', color=colors[optimizer_name])\n",
    "        plt.yticks(range(top_n), feature_labels)\n",
    "        plt.xlabel('Coefficient Magnitude')\n",
    "        plt.title(f'{optimizer_name} Feature Importance')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #Non-zero features evolution plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for optimizer_name, results in model_results.items():\n",
    "        nonzero_history = results['nonzero_history']\n",
    "        iterations = np.arange(1, len(nonzero_history) + 1)\n",
    "        plt.plot(iterations, nonzero_history, label=f\"{optimizer_name}\", color=colors[optimizer_name], linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Iterations', fontsize=12)\n",
    "    plt.ylabel('Number of Non-Zero Coefficients', fontsize=12)\n",
    "    plt.title('Sparsity Evolution During Training', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_optimizer_comparison(model_results, test_metrics, metrics_to_compare=None):\n",
    "    \"\"\"\n",
    "    Plot a comprehensive comparison of all optimizer performances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_results : dict\n",
    "        Dictionary containing all optimizer models and their results\n",
    "    test_metrics : dict\n",
    "        Dictionary containing test metrics for all optimizer models\n",
    "    metrics_to_compare : list, optional\n",
    "        List of metrics to compare ('MAE', 'RMSE', 'Mean Deviance', 'Sparsity')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plots)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    if metrics_to_compare is None:\n",
    "        metrics_to_compare = ['MAE', 'RMSE', 'Mean Deviance', 'Sparsity']\n",
    "    \n",
    "    optimizer_names = list(model_results.keys())\n",
    "    \n",
    "    # Preparing data for radar chart\n",
    "    data = []\n",
    "    for optimizer in optimizer_names:\n",
    "        row = []\n",
    "        for metric in metrics_to_compare:\n",
    "            if metric in test_metrics[optimizer]:\n",
    "                # For all metrics except Sparsity, lower is better\n",
    "                if metric != 'Sparsity':\n",
    "                    row.append(test_metrics[optimizer][metric])\n",
    "                else:\n",
    "                    # For Sparsity, higher is better (inverted)\n",
    "                    row.append(1 - test_metrics[optimizer][metric])\n",
    "        data.append(row)\n",
    "    \n",
    "    # Converting to numpy array\n",
    "    data = np.array(data)\n",
    "    \n",
    "    # Normalize the data between 0 and 1 for radar chart\n",
    "    data_normalized = np.zeros_like(data, dtype=float)\n",
    "    for i in range(len(metrics_to_compare)):\n",
    "        if metrics_to_compare[i] != 'Sparsity':\n",
    "            # For error metrics, smaller is better\n",
    "            data_normalized[:, i] = (data[:, i] - np.min(data[:, i])) / (np.max(data[:, i]) - np.min(data[:, i]) + 1e-10)\n",
    "        else:\n",
    "            # For sparsity===> inverted, so smaller is better\n",
    "            data_normalized[:, i] = (data[:, i] - np.min(data[:, i])) / (np.max(data[:, i]) - np.min(data[:, i]) + 1e-10)\n",
    "    \n",
    "    # Number of variables\n",
    "    N = len(metrics_to_compare)\n",
    "    \n",
    "    # figure for the radar chart\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Plotting the radar chart\n",
    "    angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the polygon\n",
    "    \n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Adding variable labels\n",
    "    plt.xticks(angles[:-1], metrics_to_compare, size=12)\n",
    "    \n",
    "    # y-labels (percentages)\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([0.25, 0.5, 0.75], [\"25%\", \"50%\", \"75%\"], color=\"grey\", size=10)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # colors\n",
    "    colors = {'AMGD': '#3498db', 'Adam': '#e74c3c', 'AdaGrad': '#2ecc71', 'GLMnet': '#9b59b6'}\n",
    "    for i, optimizer in enumerate(optimizer_names):\n",
    "        values = data_normalized[i].tolist()\n",
    "        values += values[:1]  # Close the polygon\n",
    "        ax.plot(angles, values, linewidth=2, linestyle='solid', label=optimizer, color=colors[optimizer])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[optimizer])\n",
    "    \n",
    "    #Legend\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Optimizer Performance Comparison\\n(Closer to center is better)\", size=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # summary table for runtime comparison\n",
    "    runtimes = [model_results[opt]['runtime'] for opt in optimizer_names]\n",
    "    iterations = [len(model_results[opt]['loss_history']) for opt in optimizer_names]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Runtime comparison\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    bar_colors = [colors[opt] for opt in optimizer_names]\n",
    "    bars = plt.bar(optimizer_names, runtimes, color=bar_colors)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + (height * 0.02),\n",
    "                f'{height:.2f}s', ha='center', va='bottom', fontsize=10)\n",
    "    plt.ylabel('Runtime (seconds)', fontsize=12)\n",
    "    plt.title('Total Runtime Comparison', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Iterations comparison\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars = plt.bar(optimizer_names, iterations, color=bar_colors)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + (height * 0.02),\n",
    "                f'{height}', ha='center', va='bottom', fontsize=10)\n",
    "    plt.ylabel('Number of Iterations', fontsize=12)\n",
    "    plt.title('Convergence Iterations Comparison', fontsize=14)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "## Statististical significance analysis \n",
    "\n",
    "\n",
    "def statistical_significance_analysis(X, y, best_params, n_bootstrap=1000, n_runs=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform statistical significance analysis for algorithm performance comparison\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix\n",
    "    y : numpy.ndarray\n",
    "        Target values\n",
    "    best_params : dict\n",
    "        Dictionary with best parameters for each optimizer\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap samples for confidence intervals\n",
    "    n_runs : int\n",
    "        Number of runs for statistical tests\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    significance_results : dict\n",
    "        Dictionary with statistical test results\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Include GLMnet in the optimizers list\n",
    "    optimizers = ['AMGD', 'Adam', 'AdaGrad', 'GLMnet']\n",
    "    metrics = ['MAE', 'RMSE', 'Mean Deviance', 'Sparsity']\n",
    "    optimizer_functions = {\n",
    "        'AMGD': amgd, \n",
    "        'Adam': adam, \n",
    "        'AdaGrad': adagrad, \n",
    "        'GLMnet': glmnet\n",
    "    }\n",
    "    \n",
    "    # 1. Split data for bootstrap validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # 2. Prepare optimizers with their best parameters (using MAE for this example)\n",
    "    optimizer_configs = {}\n",
    "    for optimizer_name in optimizers:\n",
    "        params = best_params[f\"{optimizer_name}_MAE\"]\n",
    "        reg_type = params['Regularization']\n",
    "        lambda_val = params['Lambda']\n",
    "        \n",
    "        if optimizer_name == \"AMGD\":\n",
    "            base_params = {\"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \"T\": 20.0, \n",
    "                          \"tol\": 1e-6, \"max_iter\": 1000, \"eta\": 0.0001, \"epsilon\": 1e-8}\n",
    "        elif optimizer_name == \"Adam\":\n",
    "            base_params = {\"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \n",
    "                          \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8}\n",
    "        elif optimizer_name == 'GLMnet':\n",
    "            base_params = {\"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8}\n",
    "        else:  # AdaGrad\n",
    "            base_params = {\"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 1000, \"epsilon\": 1e-8}\n",
    "        \n",
    "        if reg_type == \"L1\":\n",
    "            base_params[\"lambda1\"] = lambda_val\n",
    "            base_params[\"lambda2\"] = 0.0\n",
    "            base_params[\"penalty\"] = \"l1\"\n",
    "        else:  # ElasticNet\n",
    "            base_params[\"lambda1\"] = lambda_val / 2\n",
    "            base_params[\"lambda2\"] = lambda_val / 2\n",
    "            base_params[\"penalty\"] = \"elasticnet\"\n",
    "        \n",
    "        optimizer_configs[optimizer_name] = base_params\n",
    "    \n",
    "    # 3. Multiple runs for performance metrics\n",
    "    all_metrics = {opt: {metric: [] for metric in metrics} for opt in optimizers}\n",
    "    feature_selection = {opt: [] for opt in optimizers}\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        # Create a bootstrapped sample\n",
    "        indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_boot, y_boot = X_train[indices], y_train[indices]\n",
    "        \n",
    "        # Train each optimizer on bootstrapped data\n",
    "        for optimizer_name in optimizers:\n",
    "            try:\n",
    "                # Try with the standard 4-value return format\n",
    "                beta, _, runtime, _ = optimizer_functions[optimizer_name](\n",
    "                    X_boot, y_boot, \n",
    "                    **optimizer_configs[optimizer_name],\n",
    "                    verbose=False,\n",
    "                    return_iters=False\n",
    "                )\n",
    "            except Exception as e:\n",
    "                try:\n",
    "                    # Try with GLMnet's 5-value return format\n",
    "                    beta, _, runtime, _, _ = optimizer_functions[optimizer_name](\n",
    "                        X_boot, y_boot, \n",
    "                        **optimizer_configs[optimizer_name],\n",
    "                        verbose=False,\n",
    "                        return_iters=True\n",
    "                    )\n",
    "                except Exception as e2:\n",
    "                    print(f\"Error training {optimizer_name}: {str(e2)}\")\n",
    "                    # Create a default array of zeros as fallback\n",
    "                    beta = np.zeros(X_boot.shape[1])\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            metrics_values = evaluate_model(beta, X_test, y_test)\n",
    "            \n",
    "            # Store metrics\n",
    "            for metric in metrics:\n",
    "                all_metrics[optimizer_name][metric].append(metrics_values[metric])\n",
    "            \n",
    "            # Store feature selection (which features have non-zero coefficients)\n",
    "            feature_selection[optimizer_name].append(np.abs(beta) > 1e-6)\n",
    "    \n",
    "    # 4. Compute bootstrap statistics\n",
    "    bootstrap_results = {}\n",
    "    for optimizer_name in optimizers:\n",
    "        bootstrap_results[optimizer_name] = {}\n",
    "        \n",
    "        for metric in metrics:\n",
    "            values = all_metrics[optimizer_name][metric]\n",
    "            \n",
    "            # Bootstrap for confidence intervals\n",
    "            bootstrap_means = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                bootstrap_sample = np.random.choice(values, size=len(values), replace=True)\n",
    "                bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "            \n",
    "            # Compute statistics\n",
    "            mean_value = np.mean(values)\n",
    "            std_value = np.std(values)\n",
    "            \n",
    "            # 95% confidence interval\n",
    "            ci_lower = np.percentile(bootstrap_means, 2.5)\n",
    "            ci_upper = np.percentile(bootstrap_means, 97.5)\n",
    "            \n",
    "            bootstrap_results[optimizer_name][metric] = {\n",
    "                'mean': mean_value,\n",
    "                'std': std_value,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'values': values\n",
    "            }\n",
    "    \n",
    "    # 5. Statistical tests\n",
    "    statistical_tests = {}\n",
    "    \n",
    "    # Compare AMGD (reference) vs other optimizers\n",
    "    reference = 'AMGD'\n",
    "    for metric in metrics:\n",
    "        statistical_tests[metric] = {}\n",
    "        \n",
    "        reference_values = all_metrics[reference][metric]\n",
    "        \n",
    "        for optimizer_name in [opt for opt in optimizers if opt != reference]:\n",
    "            comparison_values = all_metrics[optimizer_name][metric]\n",
    "            \n",
    "            # Paired t-test\n",
    "            t_stat, p_value = stats.ttest_rel(reference_values, comparison_values)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            mean_diff = np.mean(reference_values) - np.mean(comparison_values)\n",
    "            pooled_std = np.sqrt((np.var(reference_values) + np.var(comparison_values)) / 2)\n",
    "            effect_size = mean_diff / pooled_std\n",
    "            \n",
    "            statistical_tests[metric][optimizer_name] = {\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'effect_size': effect_size,\n",
    "                'significant': p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    # 6. Feature selection analysis\n",
    "    feature_selection_results = {}\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    for optimizer_name in optimizers:\n",
    "        feature_selection_probs = np.mean(feature_selection[optimizer_name], axis=0)\n",
    "        \n",
    "        # For each feature, compute probability of selection\n",
    "        features = []\n",
    "        for idx in range(n_features):\n",
    "            features.append({\n",
    "                'feature_idx': idx,\n",
    "                'selection_probability': feature_selection_probs[idx]\n",
    "            })\n",
    "        \n",
    "        # Sort by selection probability\n",
    "        features = sorted(features, key=lambda x: x['selection_probability'], reverse=True)\n",
    "        \n",
    "        feature_selection_results[optimizer_name] = features\n",
    "    \n",
    "    # Combine all results\n",
    "    significance_results = {\n",
    "        'bootstrap_results': bootstrap_results,\n",
    "        'statistical_tests': statistical_tests,\n",
    "        'feature_selection_results': feature_selection_results\n",
    "    }\n",
    "    \n",
    "    return significance_results\n",
    "\n",
    "def display_statistical_results(significance_results, feature_names=None):\n",
    "    \"\"\"\n",
    "    Display the statistical significance results in a readable format\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    significance_results : dict\n",
    "        Results from statistical_significance_analysis\n",
    "    feature_names : list\n",
    "        Names of features for better readability\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Include GLMnet color\n",
    "    colors = {\n",
    "        'AMGD': '#3498db', \n",
    "        'Adam': '#e74c3c', \n",
    "        'AdaGrad': '#2ecc71', \n",
    "        'GLMnet': '#9b59b6'\n",
    "    }\n",
    "    \n",
    "    # Print summary of bootstrap results\n",
    "    print(\"\\n===== BOOTSTRAP RESULTS (95% Confidence Intervals) =====\")\n",
    "    print(\"{:<10} {:<15} {:<15} {:<15} {:<15}\".format(\n",
    "        \"Metric\", \"AMGD\", \"Adam\", \"AdaGrad\", \"GLMnet\"))\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for metric in ['MAE', 'RMSE', 'Mean Deviance', 'Sparsity']:\n",
    "        print(\"{:<10}\".format(metric), end=\" \")\n",
    "        \n",
    "        for optimizer in ['AMGD', 'Adam', 'AdaGrad', 'GLMnet']:\n",
    "            result = significance_results['bootstrap_results'][optimizer][metric]\n",
    "            print(\"{:.4f} [{:.4f}, {:.4f}]\".format(\n",
    "                result['mean'], result['ci_lower'], result['ci_upper']), end=\" \")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Print summary of statistical tests (comparing AMGD vs others)\n",
    "    print(\"\\n===== STATISTICAL TESTS (AMGD vs Others) =====\")\n",
    "    print(\"{:<10} {:<15} {:<15} {:<15}\".format(\n",
    "        \"Metric\", \"vs Adam\", \"vs AdaGrad\", \"vs GLMnet\"))\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for metric in ['MAE', 'RMSE', 'Mean Deviance', 'Sparsity']:\n",
    "        print(\"{:<10}\".format(metric), end=\" \")\n",
    "        \n",
    "        for optimizer in ['Adam', 'AdaGrad', 'GLMnet']:\n",
    "            test = significance_results['statistical_tests'][metric][optimizer]\n",
    "            result = \"p={:.4f} (d={:.2f}){}\".format(\n",
    "                test['p_value'], \n",
    "                test['effect_size'], \n",
    "                \"*\" if test['significant'] else \"\")\n",
    "            print(\"{:<15}\".format(result), end=\" \")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Print top selected features from AMGD\n",
    "    print(\"\\n===== TOP SELECTED FEATURES (AMGD) =====\")\n",
    "    amgd_features = significance_results['feature_selection_results']['AMGD']\n",
    "    \n",
    "    for i, feature in enumerate(amgd_features[:10]):  # Show top 10\n",
    "        if feature_names and feature['feature_idx'] < len(feature_names):\n",
    "            feature_name = feature_names[feature['feature_idx']]\n",
    "        else:\n",
    "            feature_name = f\"Feature {feature['feature_idx']}\"\n",
    "        \n",
    "        print(\"{:<4} {:<30} Selection Probability: {:.2f}\".format(\n",
    "            f\"{i+1}.\", feature_name, feature['selection_probability']))\n",
    "\n",
    "\n",
    "def add_to_pipeline(X, y, best_params, feature_names, model_results):\n",
    "    \"\"\"\n",
    "    Add statistical significance analysis to pipeline with fixed feature_selection handling\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Include GLMnet in colors\n",
    "    colors = {\n",
    "        'AMGD': '#3498db', \n",
    "        'Adam': '#e74c3c', \n",
    "        'AdaGrad': '#2ecc71', \n",
    "        'GLMnet': '#9b59b6'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nStep 8: Performing statistical significance analysis\")\n",
    "    significance_results = statistical_significance_analysis(X, y, best_params)\n",
    "    display_statistical_results(significance_results, feature_names)\n",
    "    \n",
    "    # Create a figure to visualize confidence intervals for metrics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    metrics = ['MAE', 'RMSE', 'Mean Deviance', 'Sparsity']\n",
    "    optimizers = ['AMGD', 'Adam', 'AdaGrad', 'GLMnet']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        \n",
    "        x_pos = np.arange(len(optimizers))\n",
    "        means = []\n",
    "        errors = []\n",
    "        \n",
    "        for j, optimizer in enumerate(optimizers):\n",
    "            result = significance_results['bootstrap_results'][optimizer][metric]\n",
    "            means.append(result['mean'])\n",
    "            errors.append([result['mean'] - result['ci_lower'], result['ci_upper'] - result['mean']])\n",
    "        \n",
    "        errors = np.array(errors).T\n",
    "        \n",
    "        bars = plt.bar(x_pos, means, color=[colors[opt] for opt in optimizers], \n",
    "                      yerr=errors, capsize=10, alpha=0.7)\n",
    "        \n",
    "        # Add value labels on top of bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + errors[1][bars.index(bar)],\n",
    "                   f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.title(f'{metric} with 95% Confidence Intervals')\n",
    "        plt.xticks(x_pos, optimizers)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # If this is sparsity, highlight that higher is better\n",
    "        if metric == 'Sparsity':\n",
    "            plt.ylabel(f'{metric} (higher is better)')\n",
    "        else:\n",
    "            plt.ylabel(f'{metric} (lower is better)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metric_confidence_intervals.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a figure to visualize feature selection consistency\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Find top 10 features based on selection probability in AMGD\n",
    "    amgd_features = significance_results['feature_selection_results']['AMGD']\n",
    "    top_features = sorted(amgd_features, key=lambda x: x['selection_probability'], reverse=True)[:10]\n",
    "    top_indices = [f['feature_idx'] for f in top_features]\n",
    "    \n",
    "    x_pos = np.arange(len(top_indices))\n",
    "    width = 0.2  # Adjusted width to fit 4 optimizers\n",
    "    \n",
    "    for i, optimizer in enumerate(optimizers):\n",
    "        features = significance_results['feature_selection_results'][optimizer]\n",
    "        probs = [next(f for f in features if f['feature_idx'] == idx)['selection_probability'] for idx in top_indices]\n",
    "        \n",
    "        plt.bar(x_pos + (i-1.5)*width, probs, width, color=colors[optimizer], label=optimizer, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Selection Probability')\n",
    "    plt.title('Feature Selection Consistency Across Optimizers')\n",
    "    \n",
    "    x_labels = []\n",
    "    for idx in top_indices:\n",
    "        if feature_names and idx < len(feature_names):\n",
    "            x_labels.append(feature_names[idx])\n",
    "        else:\n",
    "            x_labels.append(f'Feature {idx}')\n",
    "    \n",
    "    plt.xticks(x_pos, x_labels, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_selection_consistency.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a violin plot to show distribution of metrics across runs\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        \n",
    "        data = []\n",
    "        for optimizer in optimizers:\n",
    "            data.append(significance_results['bootstrap_results'][optimizer][metric]['values'])\n",
    "        \n",
    "        violin_parts = plt.violinplot(data, showmeans=True, showmedians=True)\n",
    "        \n",
    "        # Color each violin\n",
    "        for j, pc in enumerate(violin_parts['bodies']):\n",
    "            pc.set_facecolor(colors[optimizers[j]])\n",
    "            pc.set_alpha(0.7)\n",
    "        \n",
    "        # Add data points\n",
    "        for j, d in enumerate(data):\n",
    "            plt.scatter([j+1] * len(d), d, color=colors[optimizers[j]], \n",
    "                      alpha=0.2, s=5)\n",
    "        \n",
    "        plt.xticks(np.arange(1, len(optimizers)+1), optimizers)\n",
    "        plt.title(f'Distribution of {metric} Across Runs')\n",
    "        \n",
    "        # If this is sparsity, highlight that higher is better\n",
    "        if metric == 'Sparsity':\n",
    "            plt.ylabel(f'{metric} (higher is better)')\n",
    "        else:\n",
    "            plt.ylabel(f'{metric} (lower is better)')\n",
    "        \n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metric_distributions.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return significance_results\n",
    "\n",
    "def statistical_significance_analysis(X, y, best_params, n_bootstrap=500, n_runs=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform statistical significance analysis for algorithm performance comparison\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix\n",
    "    y : numpy.ndarray\n",
    "        Target values\n",
    "    best_params : dict\n",
    "        Dictionary with best parameters for each optimizer\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap samples for confidence intervals\n",
    "    n_runs : int\n",
    "        Number of runs for statistical tests\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    significance_results : dict\n",
    "        Dictionary with statistical test results\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Include GLMnet in the optimizers list\n",
    "    optimizers = ['AMGD', 'Adam', 'AdaGrad', 'GLMnet']\n",
    "    metrics = ['MAE', 'RMSE', 'Mean Deviance', 'Sparsity']\n",
    "    optimizer_functions = {\n",
    "        'AMGD': amgd, \n",
    "        'Adam': adam, \n",
    "        'AdaGrad': adagrad, \n",
    "        'GLMnet': glmnet\n",
    "    }\n",
    "    \n",
    "    # 1. Split data for bootstrap validation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # 2. Prepare optimizers with their best parameters (using MAE for this example)\n",
    "    optimizer_configs = {}\n",
    "    for optimizer_name in optimizers:\n",
    "        params = best_params[f\"{optimizer_name}_MAE\"]\n",
    "        reg_type = params['Regularization']\n",
    "        lambda_val = params['Lambda']\n",
    "        \n",
    "        if optimizer_name == \"AMGD\":\n",
    "            base_params = {\"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \"T\": 20.0, \n",
    "                          \"tol\": 1e-6, \"max_iter\": 100, \"eta\": 0.0001, \"epsilon\": 1e-8}\n",
    "        elif optimizer_name == \"Adam\":\n",
    "            base_params = {\"alpha\": 0.01, \"beta1\": 0.9, \"beta2\": 0.999, \n",
    "                          \"tol\": 1e-6, \"max_iter\": 100, \"epsilon\": 1e-8}\n",
    "        elif optimizer_name == 'GLMnet':\n",
    "            base_params = {\"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 100, \"epsilon\": 1e-8}\n",
    "        else:  # AdaGrad\n",
    "            base_params = {\"alpha\": 0.01, \"tol\": 1e-6, \"max_iter\": 100, \"epsilon\": 1e-8}\n",
    "        \n",
    "        if reg_type == \"L1\":\n",
    "            base_params[\"lambda1\"] = lambda_val\n",
    "            base_params[\"lambda2\"] = 0.0\n",
    "            base_params[\"penalty\"] = \"l1\"\n",
    "        else:  # ElasticNet\n",
    "            base_params[\"lambda1\"] = lambda_val / 2\n",
    "            base_params[\"lambda2\"] = lambda_val / 2\n",
    "            base_params[\"penalty\"] = \"elasticnet\"\n",
    "        \n",
    "        optimizer_configs[optimizer_name] = base_params\n",
    "    \n",
    "    # 3. Multiple runs for performance metrics\n",
    "    all_metrics = {opt: {metric: [] for metric in metrics} for opt in optimizers}\n",
    "    feature_selection_data = {opt: [] for opt in optimizers}  # Store feature selection data\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"Running statistical analysis iteration {run+1}/{n_runs}\")\n",
    "        # Create a bootstrapped sample\n",
    "        indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_boot, y_boot = X_train[indices], y_train[indices]\n",
    "        \n",
    "        # Train each optimizer on bootstrapped data\n",
    "        for optimizer_name in optimizers:\n",
    "            try:\n",
    "                # Try with the standard 4-value return format\n",
    "                beta, _, runtime, _ = optimizer_functions[optimizer_name](\n",
    "                    X_boot, y_boot, \n",
    "                    **optimizer_configs[optimizer_name],\n",
    "                    verbose=False,\n",
    "                    return_iters=False\n",
    "                )\n",
    "            except Exception as e:\n",
    "                try:\n",
    "                    # Try with GLMnet's 5-value return format\n",
    "                    beta, _, runtime, _, _ = optimizer_functions[optimizer_name](\n",
    "                        X_boot, y_boot, \n",
    "                        **optimizer_configs[optimizer_name],\n",
    "                        verbose=False,\n",
    "                        return_iters=True\n",
    "                    )\n",
    "                except Exception as e2:\n",
    "                    print(f\"Error training {optimizer_name}: {str(e2)}\")\n",
    "                    # Create a default array of zeros as fallback\n",
    "                    beta = np.zeros(X_boot.shape[1])\n",
    "            \n",
    "            # Evaluate on test data\n",
    "            metrics_values = evaluate_model(beta, X_test, y_test)\n",
    "            \n",
    "            # Store metrics\n",
    "            for metric in metrics:\n",
    "                all_metrics[optimizer_name][metric].append(metrics_values[metric])\n",
    "            \n",
    "            # Store feature selection (which features have non-zero coefficients)\n",
    "            feature_selection_data[optimizer_name].append(np.abs(beta) > 1e-6)\n",
    "    \n",
    "    # 4. Compute bootstrap statistics\n",
    "    bootstrap_results = {}\n",
    "    for optimizer_name in optimizers:\n",
    "        bootstrap_results[optimizer_name] = {}\n",
    "        \n",
    "        for metric in metrics:\n",
    "            values = all_metrics[optimizer_name][metric]\n",
    "            \n",
    "            # Bootstrap for confidence intervals\n",
    "            bootstrap_means = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                bootstrap_sample = np.random.choice(values, size=len(values), replace=True)\n",
    "                bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "            \n",
    "            # Compute statistics\n",
    "            mean_value = np.mean(values)\n",
    "            std_value = np.std(values)\n",
    "            \n",
    "            # 95% confidence interval\n",
    "            ci_lower = np.percentile(bootstrap_means, 2.5)\n",
    "            ci_upper = np.percentile(bootstrap_means, 97.5)\n",
    "            \n",
    "            bootstrap_results[optimizer_name][metric] = {\n",
    "                'mean': mean_value,\n",
    "                'std': std_value,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'values': values\n",
    "            }\n",
    "    \n",
    "    # 5. Statistical tests\n",
    "    statistical_tests = {}\n",
    "    \n",
    "    # Compare AMGD (reference) vs other optimizers\n",
    "    reference = 'AMGD'\n",
    "    for metric in metrics:\n",
    "        statistical_tests[metric] = {}\n",
    "        \n",
    "        reference_values = all_metrics[reference][metric]\n",
    "        \n",
    "        for optimizer_name in [opt for opt in optimizers if opt != reference]:\n",
    "            comparison_values = all_metrics[optimizer_name][metric]\n",
    "            \n",
    "            # Paired t-test\n",
    "            t_stat, p_value = stats.ttest_rel(reference_values, comparison_values)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            mean_diff = np.mean(reference_values) - np.mean(comparison_values)\n",
    "            pooled_std = np.sqrt((np.var(reference_values) + np.var(comparison_values)) / 2)\n",
    "            effect_size = mean_diff / (pooled_std + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "            \n",
    "            statistical_tests[metric][optimizer_name] = {\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'effect_size': effect_size,\n",
    "                'significant': p_value < 0.05\n",
    "            }\n",
    "    \n",
    "    # 6. Feature selection analysis\n",
    "    feature_selection_results = {}\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    for optimizer_name in optimizers:\n",
    "        # Check if feature_selection_data[optimizer_name] is not empty\n",
    "        if feature_selection_data[optimizer_name]:\n",
    "            # Calculate the mean of the feature selection across all runs\n",
    "            feature_selection_probs = np.mean(feature_selection_data[optimizer_name], axis=0)\n",
    "        else:\n",
    "            # If no data is available, initialize with zeros\n",
    "            feature_selection_probs = np.zeros(n_features)\n",
    "        \n",
    "        # For each feature, compute probability of selection\n",
    "        features = []\n",
    "        for idx in range(n_features):\n",
    "            features.append({\n",
    "                'feature_idx': idx,\n",
    "                'selection_probability': feature_selection_probs[idx]\n",
    "            })\n",
    "        \n",
    "        # Sort by selection probability\n",
    "        features = sorted(features, key=lambda x: x['selection_probability'], reverse=True)\n",
    "        \n",
    "        feature_selection_results[optimizer_name] = features\n",
    "    \n",
    "    # Combine all results\n",
    "    significance_results = {\n",
    "        'bootstrap_results': bootstrap_results,\n",
    "        'statistical_tests': statistical_tests,\n",
    "        'feature_selection_results': feature_selection_results\n",
    "    }\n",
    "    \n",
    "    return significance_results\n",
    "\n",
    "\n",
    "# Main function to run pipeline \n",
    "def run_poisson_regression_pipeline(filepath=\"ecological_health_dataset.csv\", \n",
    "                                   random_state=42, k_folds=5,\n",
    "                                   lambda_values=None, \n",
    "                                   metric_to_optimize='MAE',\n",
    "                                   plot_coefficients=True,\n",
    "                                   plot_metrics=True,\n",
    "                                   optimizers=None):\n",
    "    \"\"\"\n",
    "    Run the Poisson regression pipeline with L1 and ElasticNet regularization only:\n",
    "    1. Load and preprocess data\n",
    "    2. Split data into train, validation, and test sets\n",
    "    3. Find optimal parameters using k-fold cross-validation on validation set\n",
    "    4. Create comparison barplots for the algorithms\n",
    "    5. Train model on training set using best parameters\n",
    "    6. Compare convergence rates of the algorithms\n",
    "    7. Evaluate model on test set\n",
    "    8. Plot coefficient paths and evolution (if enabled)\n",
    "    9. Plot comprehensive metric visualizations (if enabled)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the dataset CSV file\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    k_folds : int\n",
    "        Number of folds for cross-validation\n",
    "    lambda_values : list, optional\n",
    "        List of lambda values to try\n",
    "    metric_to_optimize : str\n",
    "        Metric to use for selecting best parameters ('MAE', 'RMSE', 'Mean_Deviance', 'Runtime')\n",
    "    plot_coefficients : bool\n",
    "        Whether to generate coefficient path and evolution plots\n",
    "    plot_metrics : bool\n",
    "        Whether to generate comprehensive metric visualizations\n",
    "    optimizers : list, optional\n",
    "        List of optimizers to use ('AMGD', 'Adam', 'AdaGrad', 'GLMnet'). If None, all optimizers will be used.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    best_params : dict\n",
    "        Best parameters found through cross-validation\n",
    "    test_metrics : dict\n",
    "        Evaluation metrics on test set\n",
    "    model_results : dict\n",
    "        Results for all models including coefficients and performance\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"POISSON REGRESSION PIPELINE (L1 AND ELASTICNET)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Set default optimizers if not provided\n",
    "    if optimizers is None:\n",
    "        optimizers = ['AMGD', 'Adam', 'AdaGrad', 'GLMnet']\n",
    "    \n",
    "    # Validating optimizers\n",
    "    valid_optimizers = ['AMGD', 'Adam', 'AdaGrad', 'GLMnet']\n",
    "    for opt in optimizers:\n",
    "        if opt not in valid_optimizers:\n",
    "            raise ValueError(f\"Invalid optimizer: {opt}. Valid options are: {valid_optimizers}\")\n",
    "    \n",
    "    # 1. Load and preprocess data\n",
    "    print(\"\\nStep 1: Loading and preprocessing data\")\n",
    "    X, y, feature_names = preprocess_ecological_dataset(filepath)\n",
    "    \n",
    "    # 2. Splitting data into train, validation, and test sets (70/15/15)\n",
    "    print(\"\\nStep 2: Splitting data into train, validation, and test sets (70/15/15)\")\n",
    "    # First split: 85% train+val, 15% test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Second split: 70% train, 15% validation (82.35% of train_val is train)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.1765, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/X.shape[0]:.1%})\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/X.shape[0]:.1%})\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/X.shape[0]:.1%})\")\n",
    "    \n",
    "    # 3. Find optimal parameters using k-fold cross-validation on validation set\n",
    "    print(f\"\\nStep 3: Finding optimal parameters using {k_folds}-fold cross-validation on validation set\")\n",
    "    print(f\"Regularization with L1 and ElasticNet using optimizers: {optimizers}\")\n",
    "    \n",
    "    if lambda_values is None:\n",
    "        lambda_values = np.logspace(-4, np.log10(20), 20)  \n",
    "\n",
    "        #lambda_values = np.logspace(-4, 1, 6)  # [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "        print(f\"Using lambda values: {lambda_values}\")\n",
    "    \n",
    "    # optimizers_to_use\n",
    "    best_params, cv_results_df = k_fold_cross_validation(\n",
    "        X_val, y_val, k=k_folds, lambda_values=lambda_values, seed=random_state,\n",
    "        optimizers_to_use=optimizers  # Fixed: optimizers -> optimizers_to_use\n",
    "    )\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(\"\\nBest parameters found through cross-validation:\")\n",
    "    for metric, params in best_params.items():\n",
    "        if metric.startswith('Overall_Best_'):\n",
    "            metric_name = metric.replace('Overall_Best_', '')\n",
    "            print(f\"Best for {metric_name}: {params['Optimizer']} with {params['Regularization']} (λ={params['Lambda']:.6f}), Value: {params['Metric_Value']:.6f}\")\n",
    "    \n",
    "    # 4.  comparison barplots for the algorithms\n",
    "    print(\"\\nStep 4: Creating algorithm performance comparison plots\")\n",
    "    comparison_plots = create_algorithm_comparison_plots(cv_results_df)\n",
    "    \n",
    "    # Display comparison plots\n",
    "    for fig in comparison_plots:\n",
    "        plt.figure(fig.number)\n",
    "        plt.show()\n",
    "    \n",
    "    # 5. Train model on training set using best parameters\n",
    "    print(f\"\\nStep 5: Training model on training set using best parameters for {metric_to_optimize}\")\n",
    "    model_results = train_all_optimizers(\n",
    "        X_train, y_train, best_params, metric=metric_to_optimize\n",
    "    )\n",
    "    \n",
    "    # Selecting the best optimizer based on the chosen metric\n",
    "    best_optimizer = best_params[f'Overall_Best_{metric_to_optimize}']['Optimizer']\n",
    "    print(f\"Using {best_optimizer} model for test evaluation (best for {metric_to_optimize})\")\n",
    "    \n",
    "    # 6. Comparing convergence rates of the algorithms\n",
    "    print(\"\\nStep 6: Comparing convergence rates of optimization algorithms\")\n",
    "    # Check if compare_convergence_rates accepts an optimizers parameter\n",
    "    try:\n",
    "        convergence_plot = compare_convergence_rates(\n",
    "            X_train, y_train, best_params\n",
    "        )\n",
    "    except TypeError:\n",
    "        # If the function doesn't accept an optimizers parameter, catch the error\n",
    "        print(\"Note: Using all optimizers for convergence rate comparison\")\n",
    "        convergence_plot = compare_convergence_rates(\n",
    "            X_train, y_train, best_params\n",
    "        )\n",
    "    \n",
    "    plt.figure(convergence_plot.number)\n",
    "    plt.show()\n",
    "    \n",
    "    # 7. Evaluating all models on test set\n",
    "    print(\"\\nStep 7: Evaluating all models on test set\")\n",
    "    \n",
    "    test_metrics = {}\n",
    "    \n",
    "    # Evaluating each optimizer with its best parameters\n",
    "    for optimizer_name, results in model_results.items():\n",
    "        # Skip optimizers that weren't requested\n",
    "        if optimizer_name not in optimizers:\n",
    "            continue\n",
    "            \n",
    "        beta = results['beta']\n",
    "        metrics = evaluate_model(beta, X_test, y_test, target_name=optimizer_name)\n",
    "        test_metrics[optimizer_name] = metrics\n",
    "        \n",
    "        print(f\"\\nTest metrics for {optimizer_name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: {value}\")\n",
    "                \n",
    "    # Creating a comparison table\n",
    "    comparison_metrics = ['MAE', 'RMSE', 'Mean Deviance', 'Sparsity']\n",
    "    comparison_data = []\n",
    "    \n",
    "    for optimizer, metrics in test_metrics.items():\n",
    "        row = [optimizer]\n",
    "        for metric in comparison_metrics:\n",
    "            row.append(metrics[metric])\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data, columns=['Optimizer'] + comparison_metrics)\n",
    "    print(\"\\nModel comparison on test set:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Highlight the best model according to the chosen metric\n",
    "    best_idx = comparison_df[metric_to_optimize if metric_to_optimize in comparison_metrics else 'MAE'].idxmin()\n",
    "    best_test_optimizer = comparison_df.iloc[best_idx]['Optimizer']\n",
    "    print(f\"\\nBest model on test set for {metric_to_optimize}: {best_test_optimizer}\")\n",
    "    \n",
    "    # Visualizing feature importance for the best model\n",
    "    best_beta = model_results[best_test_optimizer]['beta']\n",
    "    \n",
    "    top_n = 15\n",
    "    importance = np.abs(best_beta)\n",
    "    indices = np.argsort(importance)[::-1]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(f'Top {top_n} Feature Importances ({best_test_optimizer} model)')\n",
    "    plt.bar(range(min(top_n, len(feature_names))), \n",
    "           importance[indices[:top_n]], \n",
    "           align='center')\n",
    "    plt.xticks(range(min(top_n, len(feature_names))), \n",
    "               [feature_names[i] for i in indices[:top_n]], \n",
    "               rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 8. Plot coefficient paths and evolution \n",
    "    if plot_coefficients:\n",
    "        print(\"\\nStep 8: Plotting coefficient paths and evolution\")\n",
    "        # Get top indices for plotting\n",
    "        top_indices = indices[:17]  # Top 17 features\n",
    "        top_feature_names = [feature_names[i] for i in top_indices]\n",
    "        \n",
    "        # Plot coefficient paths\n",
    "        print(\"Plotting coefficient paths...\")\n",
    "        try:\n",
    "            # try with the optimizers parameter\n",
    "            plot_coefficient_paths_for_ecological_data()\n",
    "        except Exception as e:\n",
    "            print(f\"Note: Could not use optimizers parameter for plotting. Using all optimizers. Error: {e}\")\n",
    "            plot_coefficient_paths_for_ecological_data()\n",
    "    \n",
    "    # 9. Plot comprehensive metric visualizations \n",
    "    if plot_metrics:\n",
    "        print(\"\\nStep 9: Creating comprehensive metric visualizations\")\n",
    "        \n",
    "        # Add feature names to model_results for plotting\n",
    "        for optimizer_name in model_results:\n",
    "            model_results[optimizer_name]['feature_names'] = feature_names\n",
    "        \n",
    "        # Plot training and test metrics\n",
    "        print(\"Plotting training and test metrics comparison...\")\n",
    "        plot_training_and_test_metrics(model_results, test_metrics, metric_to_plot=metric_to_optimize)\n",
    "        \n",
    "        # Plot radar chart and performance comparison\n",
    "        print(\"Plotting optimizer performance comparison...\")\n",
    "        plot_optimizer_comparison(model_results, test_metrics)\n",
    "\n",
    "      # 8. Statistical Significance Analysis\n",
    "        print(\"\\nStep 8: Performing Statistical Significance Analysis\")\n",
    "        significance_results = add_to_pipeline(\n",
    "        X, y, \n",
    "        best_params, \n",
    "        feature_names, \n",
    "        model_results\n",
    "        )\n",
    "    \n",
    "    print(\"\\nPipeline completed successfully!\")\n",
    "    \n",
    "    # Return best parameters, test metrics for all models, and all model results\n",
    "    return best_params, test_metrics, model_results, significance_results\n",
    "\n",
    "\n",
    "\n",
    "# execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #lambda_values = np.logspace(-4, 1, 50)  # Creates 50 values between 10^-4 and 10^1\n",
    "    lambda_values = np.logspace(-4, np.log10(20), 50)  \n",
    "    \n",
    "    try:\n",
    "        best_params, test_metrics, model_results, significance_results = run_poisson_regression_pipeline(\n",
    "        filepath=\"ecological_health_dataset.csv\",\n",
    "        random_state=42,\n",
    "        k_folds=5,\n",
    "        lambda_values=lambda_values,\n",
    "        metric_to_optimize='MAE'\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        print(\"\\nAnalysis complete. Summary of test metrics:\")\n",
    "        for metric, value in test_metrics.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"{metric}: {value}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset file not found.\")\n",
    "        print(\"Please ensure the dataset file is in the correct location.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
